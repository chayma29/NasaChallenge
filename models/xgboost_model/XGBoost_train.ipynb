{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98741f23",
   "metadata": {},
   "source": [
    "# XGBOOST EXOPLANET CLASSIFICATION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ce36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e60d81",
   "metadata": {},
   "source": [
    "# CONFIGURATION GLOBALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8fca8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBOOST - HYPERPARAMETER TUNING + FINAL TRAINING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Style des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Chemins des fichiers\n",
    "BASE_PATH = 'C:/Users/chaym/Desktop/NasaChallenge'\n",
    "DATA_PATH = f'{BASE_PATH}/data/processed'\n",
    "MODEL_PATH = f'{BASE_PATH}/models/xgboost_model'\n",
    "\n",
    "# Noms des classes\n",
    "CLASS_NAMES = {\n",
    "    0: 'False Positive',\n",
    "    1: 'Candidate',\n",
    "    2: 'Confirmed'\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST - HYPERPARAMETER TUNING + FINAL TRAINING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a22ab4",
   "metadata": {},
   "source": [
    "# VERIFICATION CUDA/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee064fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. VÉRIFICATION CUDA\n",
      "================================================================================\n",
      "XGBoost version: 2.1.4\n",
      "✅ Support GPU: ACTIVÉ\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_availability():\n",
    "    \"\"\"Vérifie la disponibilité de CUDA pour XGBoost\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"1. VÉRIFICATION CUDA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    build_info = xgb.build_info()\n",
    "    \n",
    "    if not build_info.get('USE_CUDA', False):\n",
    "        print(\"⚠️  ATTENTION: XGBoost n'a pas été compilé avec CUDA\")\n",
    "        print(\"📌 Le modèle utilisera le CPU\")\n",
    "        device = 'cpu'\n",
    "    else:\n",
    "        print(\"✅ Support GPU: ACTIVÉ\")\n",
    "        device = 'cuda'\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = check_cuda_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb023710",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08712bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. CHARGEMENT DES DONNÉES\n",
      "================================================================================\n",
      "\n",
      "📊 Dimensions des datasets:\n",
      "   Train:        6694 samples,  13 features\n",
      "   Validation:    956 samples,  13 features\n",
      "   Test:          957 samples,  13 features\n",
      "\n",
      "📈 Distribution des classes (Train):\n",
      "   False Positive      :  3387 (50.60%)\n",
      "   Candidate           :  1385 (20.69%)\n",
      "   Confirmed           :  1922 (28.71%)\n"
     ]
    }
   ],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Charge les datasets train, validation et test\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"2. CHARGEMENT DES DONNÉES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Chargement\n",
    "    X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "    y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').squeeze()\n",
    "    \n",
    "    X_validate = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "    y_validate = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').squeeze()\n",
    "    \n",
    "    X_test = pd.read_csv(f'{DATA_PATH}/step6_X_test.csv')\n",
    "    y_test = pd.read_csv(f'{DATA_PATH}/step6_y_test.csv').squeeze()\n",
    "    \n",
    "    # Affichage des dimensions\n",
    "    print(f\"\\n📊 Dimensions des datasets:\")\n",
    "    print(f\"   Train:      {X_train.shape[0]:>6} samples, {X_train.shape[1]:>3} features\")\n",
    "    print(f\"   Validation: {X_validate.shape[0]:>6} samples, {X_validate.shape[1]:>3} features\")\n",
    "    print(f\"   Test:       {X_test.shape[0]:>6} samples, {X_test.shape[1]:>3} features\")\n",
    "    \n",
    "    # Distribution des classes\n",
    "    print(f\"\\n📈 Distribution des classes (Train):\")\n",
    "    for cls in [0, 1, 2]:\n",
    "        count = (y_train == cls).sum()\n",
    "        pct = count / len(y_train) * 100\n",
    "        print(f\"   {CLASS_NAMES[cls]:20s}: {count:>5} ({pct:>5.2f}%)\")\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = load_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9bafb",
   "metadata": {},
   "source": [
    "# Features Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e1bf81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. VÉRIFICATION DES FEATURES CRITIQUES\n",
      "================================================================================\n",
      "\n",
      "🔍 Features requises:\n",
      "   ✅ koi_duration\n",
      "   ✅ koi_duration_err1\n",
      "   ✅ koi_depth\n",
      "   ✅ koi_depth_err1\n",
      "   ✅ koi_model_snr\n",
      "\n",
      "✅ Toutes les features requises sont présentes\n"
     ]
    }
   ],
   "source": [
    "def verify_required_features(X):\n",
    "    \"\"\"Vérifie la présence des features critiques\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3. VÉRIFICATION DES FEATURES CRITIQUES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    required_features = [\n",
    "        'koi_duration',\n",
    "        'koi_duration_err1',\n",
    "        'koi_depth',\n",
    "        'koi_depth_err1',\n",
    "        'koi_model_snr'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🔍 Features requises:\")\n",
    "    all_present = True\n",
    "    for feat in required_features:\n",
    "        if feat in X.columns:\n",
    "            print(f\"   ✅ {feat}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {feat} - MANQUANTE!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if not all_present:\n",
    "        raise ValueError(\"❌ Features critiques manquantes!\")\n",
    "    \n",
    "    print(\"\\n✅ Toutes les features requises sont présentes\")\n",
    "\n",
    "verify_required_features(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cefaab",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cab8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "   ✅ transit_depth_duration_ratio\n",
      "   ✅ snr_log\n",
      "   ✅ snr_squared\n",
      "   ✅ transit_detectability\n",
      "\n",
      "📊 Features créées: 4\n",
      "📊 Features totales: 17 (original: 13)\n",
      "\n",
      "================================================================================\n",
      "4. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "   ✅ transit_depth_duration_ratio\n",
      "   ✅ snr_log\n",
      "   ✅ snr_squared\n",
      "   ✅ transit_detectability\n",
      "\n",
      "📊 Features créées: 4\n",
      "📊 Features totales: 17 (original: 13)\n",
      "\n",
      "================================================================================\n",
      "4. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "   ✅ transit_depth_duration_ratio\n",
      "   ✅ snr_log\n",
      "   ✅ snr_squared\n",
      "   ✅ transit_detectability\n",
      "\n",
      "📊 Features créées: 4\n",
      "📊 Features totales: 17 (original: 13)\n"
     ]
    }
   ],
   "source": [
    "def engineer_transit_features(X):\n",
    "    \"\"\"\n",
    "    Crée des features supplémentaires basées sur les transits planétaires\n",
    "    \n",
    "    Features créées:\n",
    "    1. transit_depth_duration_ratio: Ratio profondeur/durée du transit\n",
    "    2. snr_log: Log du SNR pour normalisation\n",
    "    3. snr_squared: SNR au carré pour capturer les effets non-linéaires\n",
    "    4. transit_detectability: Index de détectabilité combinant profondeur, SNR et durée\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"4. FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X_enhanced = X.copy()\n",
    "    features_created = []\n",
    "    \n",
    "    # Feature 1: Ratio Profondeur/Durée\n",
    "    if 'koi_depth' in X.columns and 'koi_duration' in X.columns:\n",
    "        X_enhanced['transit_depth_duration_ratio'] = (\n",
    "            X['koi_depth'] / (X['koi_duration'] + 1e-6)\n",
    "        )\n",
    "        features_created.append('transit_depth_duration_ratio')\n",
    "        print(\"   ✅ transit_depth_duration_ratio\")\n",
    "    \n",
    "    # Features 2-3: Transformations du SNR\n",
    "    if 'koi_model_snr' in X.columns:\n",
    "        X_enhanced['snr_log'] = np.log1p(X['koi_model_snr'])\n",
    "        X_enhanced['snr_squared'] = X['koi_model_snr'] ** 2\n",
    "        features_created.extend(['snr_log', 'snr_squared'])\n",
    "        print(\"   ✅ snr_log\")\n",
    "        print(\"   ✅ snr_squared\")\n",
    "    \n",
    "    # Feature 4: Transit Detectability Index\n",
    "    if all(f in X.columns for f in ['koi_duration', 'koi_depth', 'koi_model_snr']):\n",
    "        X_enhanced['transit_detectability'] = (\n",
    "            X['koi_depth'] * X['koi_model_snr'] / (X['koi_duration'] + 1)\n",
    "        )\n",
    "        features_created.append('transit_detectability')\n",
    "        print(\"   ✅ transit_detectability\")\n",
    "    \n",
    "    print(f\"\\n📊 Features créées: {len(features_created)}\")\n",
    "    print(f\"📊 Features totales: {X_enhanced.shape[1]} (original: {X.shape[1]})\")\n",
    "    \n",
    "    return X_enhanced\n",
    "\n",
    "# Application du feature engineering\n",
    "X_train_enh = engineer_transit_features(X_train)\n",
    "X_val_enh = engineer_transit_features(X_validate)\n",
    "X_test_enh = engineer_transit_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e984d85",
   "metadata": {},
   "source": [
    "# CALCULATION OF CLASS WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea0729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. CALCUL DES POIDS DE CLASSES\n",
      "================================================================================\n",
      "\n",
      "⚖️  Poids calculés:\n",
      "   False Positive      : 0.5270x\n",
      "   Candidate           : 2.4166x [BOOST +50%]\n",
      "   Confirmed           : 1.1609x\n"
     ]
    }
   ],
   "source": [
    "def compute_balanced_class_weights(y):\n",
    "    \"\"\"\n",
    "    Calcule les poids de classes pour gérer le déséquilibre\n",
    "    Boost spécial pour la classe 1 (Candidate)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"5. CALCUL DES POIDS DE CLASSES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_classes = np.array([0, 1, 2])\n",
    "    auto_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=all_classes,\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    # Ajustement manuel des poids\n",
    "    class_weights = {\n",
    "        0: auto_weights[0] * 0.8,   # False Positive: légère réduction\n",
    "        1: auto_weights[1] * 1.5,   # Candidate: BOOST +50%\n",
    "        2: auto_weights[2] * 1.0    # Confirmed: poids standard\n",
    "    }\n",
    "    \n",
    "    print(\"\\n⚖️  Poids calculés:\")\n",
    "    for cls, weight in class_weights.items():\n",
    "        boost = \" [BOOST +50%]\" if cls == 1 else \"\"\n",
    "        print(f\"   {CLASS_NAMES[cls]:20s}: {weight:.4f}x{boost}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "class_weights = compute_balanced_class_weights(y_train)\n",
    "sample_weights_train = np.array([class_weights[cls] for cls in y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625c7fa",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING (Train/Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a352bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "🔬 Nombre total de combinaisons: 243\n",
      "⏳ Test en cours...\n",
      "\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.7 → val_loss=0.523210\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.8 → val_loss=0.521443\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.9 → val_loss=0.520415\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.7 → val_loss=0.519092\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.8 → val_loss=0.518899\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.9 → val_loss=0.519555\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.7 → val_loss=0.518344\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.8 → val_loss=0.516290\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.9 → val_loss=0.517444\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.7 → val_loss=0.534345\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.8 → val_loss=0.533988\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.9 → val_loss=0.533704\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.7 → val_loss=0.530052\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.8 → val_loss=0.530225\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.9 → val_loss=0.530042\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.7 → val_loss=0.528016\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.8 → val_loss=0.526972\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.9 → val_loss=0.527080\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.7 → val_loss=0.544942\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.8 → val_loss=0.543281\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.9 → val_loss=0.542684\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.7 → val_loss=0.540793\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.8 → val_loss=0.540317\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.9 → val_loss=0.539267\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.7 → val_loss=0.537439\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.8 → val_loss=0.538057\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.9 → val_loss=0.536408\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.7 → val_loss=0.517725\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.8 → val_loss=0.516957\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.9 → val_loss=0.518025\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.7 → val_loss=0.514072\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.8 → val_loss=0.513053\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.9 → val_loss=0.513738\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.7 → val_loss=0.512172\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.8 → val_loss=0.510668\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.9 → val_loss=0.510465\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.7 → val_loss=0.530534\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.8 → val_loss=0.530090\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.9 → val_loss=0.530333\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.7 → val_loss=0.526862\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.8 → val_loss=0.525662\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.9 → val_loss=0.527000\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.7 → val_loss=0.523376\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.8 → val_loss=0.523620\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.9 → val_loss=0.523219\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.7 → val_loss=0.542057\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.8 → val_loss=0.540746\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.9 → val_loss=0.541844\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.7 → val_loss=0.536400\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.8 → val_loss=0.536271\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.9 → val_loss=0.536358\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.7 → val_loss=0.534795\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.8 → val_loss=0.535115\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.9 → val_loss=0.533039\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.7 → val_loss=0.516862\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.8 → val_loss=0.515622\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.9 → val_loss=0.514689\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.7 → val_loss=0.511970\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.8 → val_loss=0.511028\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.9 → val_loss=0.512109\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.7 → val_loss=0.509031\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.8 → val_loss=0.509569\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.9 → val_loss=0.510694\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.7 → val_loss=0.529312\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.8 → val_loss=0.529819\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.9 → val_loss=0.528401\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.7 → val_loss=0.525873\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.8 → val_loss=0.523249\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.9 → val_loss=0.524425\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.7 → val_loss=0.520770\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.8 → val_loss=0.522023\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.9 → val_loss=0.521392\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.7 → val_loss=0.541296\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.8 → val_loss=0.540526\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.9 → val_loss=0.540892\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.7 → val_loss=0.535433\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.8 → val_loss=0.534119\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.9 → val_loss=0.534934\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.7 → val_loss=0.532712\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.8 → val_loss=0.532717\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.9 → val_loss=0.531733\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.7 → val_loss=0.509345\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.8 → val_loss=0.512683\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.9 → val_loss=0.511941\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.7 → val_loss=0.507573\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.8 → val_loss=0.508609\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.9 → val_loss=0.509448\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.7 → val_loss=0.510329\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.8 → val_loss=0.509047\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.9 → val_loss=0.507705\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.7 → val_loss=0.514142\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.8 → val_loss=0.519407\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.9 → val_loss=0.518357\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.7 → val_loss=0.520455\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.8 → val_loss=0.521126\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.9 → val_loss=0.517794\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.7 → val_loss=0.516266\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.8 → val_loss=0.511665\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.9 → val_loss=0.513111\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.7 → val_loss=0.521583\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.8 → val_loss=0.522687\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.9 → val_loss=0.520646\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.7 → val_loss=0.522410\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.8 → val_loss=0.521015\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.9 → val_loss=0.518961\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.7 → val_loss=0.518711\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.8 → val_loss=0.515521\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.9 → val_loss=0.521923\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.7 → val_loss=0.506914\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.8 → val_loss=0.510812\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.9 → val_loss=0.512414\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.7 → val_loss=0.504529\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.8 → val_loss=0.506274\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.9 → val_loss=0.509230\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.7 → val_loss=0.507065\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.8 → val_loss=0.507738\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.9 → val_loss=0.505840\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.7 → val_loss=0.516036\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.8 → val_loss=0.517760\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.9 → val_loss=0.515454\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.7 → val_loss=0.517715\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.8 → val_loss=0.518532\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.9 → val_loss=0.519477\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.7 → val_loss=0.514148\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.8 → val_loss=0.516024\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.9 → val_loss=0.512666\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.7 → val_loss=0.520376\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.8 → val_loss=0.523219\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.9 → val_loss=0.520854\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.7 → val_loss=0.521810\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.8 → val_loss=0.520857\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.9 → val_loss=0.523916\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.7 → val_loss=0.519606\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.8 → val_loss=0.518665\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.9 → val_loss=0.519264\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.7 → val_loss=0.506276\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.8 → val_loss=0.508332\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.9 → val_loss=0.512847\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.7 → val_loss=0.506850\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.8 → val_loss=0.509638\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.9 → val_loss=0.511307\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.7 → val_loss=0.508186\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.8 → val_loss=0.507109\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.9 → val_loss=0.507092\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.7 → val_loss=0.517436\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.8 → val_loss=0.517170\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.9 → val_loss=0.516644\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.7 → val_loss=0.517500\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.8 → val_loss=0.516849\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.9 → val_loss=0.518264\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.7 → val_loss=0.510721\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.8 → val_loss=0.513832\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.9 → val_loss=0.512439\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.7 → val_loss=0.525941\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.8 → val_loss=0.522385\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.9 → val_loss=0.518487\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.7 → val_loss=0.522729\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.8 → val_loss=0.520968\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.9 → val_loss=0.524994\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.7 → val_loss=0.517468\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.8 → val_loss=0.514668\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.9 → val_loss=0.518955\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.7 → val_loss=0.508582\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.8 → val_loss=0.513614\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.9 → val_loss=0.518098\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.7 → val_loss=0.508040\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.8 → val_loss=0.511166\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.9 → val_loss=0.514699\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.7 → val_loss=0.511378\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.8 → val_loss=0.508223\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.9 → val_loss=0.507178\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.7 → val_loss=0.515260\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.8 → val_loss=0.514723\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.9 → val_loss=0.516062\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.7 → val_loss=0.514869\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.8 → val_loss=0.514146\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.9 → val_loss=0.515882\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.7 → val_loss=0.516957\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.8 → val_loss=0.513070\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.9 → val_loss=0.513408\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.7 → val_loss=0.524180\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.8 → val_loss=0.521487\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.9 → val_loss=0.525996\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.7 → val_loss=0.517925\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.8 → val_loss=0.516406\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.9 → val_loss=0.521998\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.7 → val_loss=0.519307\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.8 → val_loss=0.519571\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.9 → val_loss=0.520746\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.7 → val_loss=0.507328\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.8 → val_loss=0.511805\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.9 → val_loss=0.511799\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.7 → val_loss=0.509388\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.8 → val_loss=0.510759\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.9 → val_loss=0.514064\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.7 → val_loss=0.505324\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.8 → val_loss=0.505815\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.9 → val_loss=0.510417\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.7 → val_loss=0.514911\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.8 → val_loss=0.517197\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.9 → val_loss=0.519240\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.7 → val_loss=0.514352\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.8 → val_loss=0.513971\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.9 → val_loss=0.519635\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.7 → val_loss=0.514835\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.8 → val_loss=0.515740\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.9 → val_loss=0.508930\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.7 → val_loss=0.522029\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.8 → val_loss=0.521244\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.9 → val_loss=0.528333\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.7 → val_loss=0.520664\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.8 → val_loss=0.515336\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.9 → val_loss=0.520387\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.7 → val_loss=0.520582\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.8 → val_loss=0.519804\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.9 → val_loss=0.521480\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.7 → val_loss=0.508665\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.8 → val_loss=0.511557\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.9 → val_loss=0.514308\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.7 → val_loss=0.510495\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.8 → val_loss=0.510984\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.9 → val_loss=0.515211\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.7 → val_loss=0.511095\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.8 → val_loss=0.508947\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.9 → val_loss=0.507731\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.7 → val_loss=0.521762\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.8 → val_loss=0.521046\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.9 → val_loss=0.519813\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.7 → val_loss=0.517285\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.8 → val_loss=0.513035\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.9 → val_loss=0.519361\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.7 → val_loss=0.511745\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.8 → val_loss=0.514544\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.9 → val_loss=0.511672\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.7 → val_loss=0.521189\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.8 → val_loss=0.520991\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.9 → val_loss=0.525502\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.7 → val_loss=0.520820\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.8 → val_loss=0.518293\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.9 → val_loss=0.519543\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.7 → val_loss=0.517154\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.8 → val_loss=0.521149\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.9 → val_loss=0.518859\n",
      "\n",
      "================================================================================\n",
      "🏆 MEILLEURS HYPERPARAMÈTRES TROUVÉS\n",
      "================================================================================\n",
      "\n",
      "🎯 Meilleur score validation: 0.504529\n",
      "\n",
      "📋 Hyperparamètres optimaux:\n",
      "   learning_rate:     0.03\n",
      "   max_depth:         10\n",
      "   min_child_weight:  20\n",
      "   subsample:         0.8\n",
      "   colsample_bytree:  0.7\n",
      "\n",
      "💾 Résultats sauvegardés: hyperparameter_tuning_results.csv\n"
     ]
    }
   ],
   "source": [
    "def perform_hyperparameter_tuning(X_train, y_train, X_val, y_val, \n",
    "                                   sample_weights, device):\n",
    "    \"\"\"\n",
    "    Effectue un Grid Search sur les hyperparamètres\n",
    "    Utilise Train pour l'entraînement et Val pour la validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Grille d'hyperparamètres\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.03, 0.05],\n",
    "        'max_depth': [8, 10, 12],\n",
    "        'min_child_weight': [20, 30, 40],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "    print(f\"\\n🔬 Nombre total de combinaisons: {total_combinations}\")\n",
    "    print(\"⏳ Test en cours...\\n\")\n",
    "    \n",
    "    # Préparation des datasets\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=sample_weights)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    # Grid Search\n",
    "    for lr, md, mcw, sub, col in product(\n",
    "        param_grid['learning_rate'],\n",
    "        param_grid['max_depth'],\n",
    "        param_grid['min_child_weight'],\n",
    "        param_grid['subsample'],\n",
    "        param_grid['colsample_bytree']\n",
    "    ):\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'tree_method': 'hist' if device == 'cpu' else 'gpu_hist',\n",
    "            'device': device,\n",
    "            'learning_rate': lr,\n",
    "            'max_depth': md,\n",
    "            'min_child_weight': mcw,\n",
    "            'subsample': sub,\n",
    "            'colsample_bytree': col,\n",
    "            'alpha': 0.1,\n",
    "            'lambda': 0.1,\n",
    "            'gamma': 0.001,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Entraînement\n",
    "        evals = [(dtrain, 'train'), (dval, 'valid')]\n",
    "        model_temp = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        val_score = model_temp.best_score\n",
    "        \n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'max_depth': md,\n",
    "            'min_child_weight': mcw,\n",
    "            'subsample': sub,\n",
    "            'colsample_bytree': col,\n",
    "            'val_score': val_score,\n",
    "            'best_iteration': model_temp.best_iteration\n",
    "        })\n",
    "        \n",
    "        if val_score < best_score:\n",
    "            best_score = val_score\n",
    "            best_params = params.copy()\n",
    "        \n",
    "        print(f\"lr={lr:.2f}, md={md}, mcw={mcw}, sub={sub:.1f}, \"\n",
    "              f\"col={col:.1f} → val_loss={val_score:.6f}\")\n",
    "    \n",
    "    # Affichage des meilleurs résultats\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🏆 MEILLEURS HYPERPARAMÈTRES TROUVÉS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n🎯 Meilleur score validation: {best_score:.6f}\")\n",
    "    print(\"\\n📋 Hyperparamètres optimaux:\")\n",
    "    print(f\"   learning_rate:     {best_params['learning_rate']}\")\n",
    "    print(f\"   max_depth:         {best_params['max_depth']}\")\n",
    "    print(f\"   min_child_weight:  {best_params['min_child_weight']}\")\n",
    "    print(f\"   subsample:         {best_params['subsample']}\")\n",
    "    print(f\"   colsample_bytree:  {best_params['colsample_bytree']}\")\n",
    "    \n",
    "    # Sauvegarde des résultats\n",
    "    results_df = pd.DataFrame(results).sort_values('val_score')\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    results_df.to_csv(f'{MODEL_PATH}/hyperparameter_tuning_results.csv', \n",
    "                      index=False)\n",
    "    print(f\"\\n💾 Résultats sauvegardés: hyperparameter_tuning_results.csv\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "best_params, best_score = perform_hyperparameter_tuning(\n",
    "    X_train_enh, y_train, X_val_enh, y_validate,\n",
    "    sample_weights_train, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b82229",
   "metadata": {},
   "source": [
    "# FINAL TRAINING (Train + Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b36732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: ENTRAÎNEMENT FINAL\n",
      "================================================================================\n",
      "\n",
      "📊 Dataset combiné (Train+Val): 7650 samples\n",
      "\n",
      "⏳ Entraînement final en cours...\n",
      "\n",
      "[0]\ttrain-mlogloss:1.07581\ttest-mlogloss:1.07869\n",
      "[100]\ttrain-mlogloss:0.45049\ttest-mlogloss:0.61102\n",
      "[200]\ttrain-mlogloss:0.35567\ttest-mlogloss:0.57265\n",
      "[300]\ttrain-mlogloss:0.30371\ttest-mlogloss:0.55954\n",
      "[400]\ttrain-mlogloss:0.26578\ttest-mlogloss:0.55399\n",
      "[500]\ttrain-mlogloss:0.23610\ttest-mlogloss:0.55258\n",
      "[589]\ttrain-mlogloss:0.21410\ttest-mlogloss:0.55353\n",
      "\n",
      "✅ Entraînement terminé!\n",
      "   Best Iteration: 490\n",
      "   Best Test Loss: 0.551759\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                      best_params, class_weights, device):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle final en combinant Train et Val\n",
    "    Évalue sur le Test Set\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: ENTRAÎNEMENT FINAL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Combiner Train et Val\n",
    "    X_train_val = pd.concat([X_train, X_val], axis=0)\n",
    "    y_train_val = pd.concat([y_train, y_val], axis=0)\n",
    "    \n",
    "    # Recalculer les poids\n",
    "    sample_weights_combined = np.array([class_weights[cls] for cls in y_train_val])\n",
    "    \n",
    "    print(f\"\\n📊 Dataset combiné (Train+Val): {X_train_val.shape[0]} samples\")\n",
    "    \n",
    "    # Préparation des datasets\n",
    "    dtrain_final = xgb.DMatrix(X_train_val, label=y_train_val, \n",
    "                                weight=sample_weights_combined)\n",
    "    dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Entraînement final\n",
    "    print(\"\\n⏳ Entraînement final en cours...\\n\")\n",
    "    \n",
    "    evals_final = [(dtrain_final, 'train'), (dtest_final, 'test')]\n",
    "    \n",
    "    model_final = xgb.train(\n",
    "        best_params,\n",
    "        dtrain_final,\n",
    "        num_boost_round=3000,\n",
    "        evals=evals_final,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Entraînement terminé!\")\n",
    "    print(f\"   Best Iteration: {model_final.best_iteration}\")\n",
    "    print(f\"   Best Test Loss: {model_final.best_score:.6f}\")\n",
    "    \n",
    "    return model_final, X_train_val, y_train_val\n",
    "\n",
    "model_final, X_train_val, y_train_val = train_final_model(\n",
    "    X_train_enh, y_train, X_val_enh, y_validate, X_test_enh, y_test,\n",
    "    best_params, class_weights, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a705863f",
   "metadata": {},
   "source": [
    "# FINAL EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23c113bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ÉVALUATION FINALE SUR TEST SET\n",
      "================================================================================\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Accuracy:           0.7513 (75.13%)\n",
      "   Precision (Avg):    0.7879\n",
      "   Recall (Avg):       0.7513\n",
      "   F1-Score (Macro):   0.7358\n",
      "   F1-Score (Weighted):0.7610\n",
      "\n",
      "📋 Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive     0.8878    0.7190    0.7945       484\n",
      "     Candidate     0.4930    0.7121    0.5826       198\n",
      "     Confirmed     0.8244    0.8364    0.8303       275\n",
      "\n",
      "      accuracy                         0.7513       957\n",
      "     macro avg     0.7350    0.7558    0.7358       957\n",
      "  weighted avg     0.7879    0.7513    0.7610       957\n",
      "\n",
      "\n",
      "🎯 PERFORMANCE CLASSE 1 (Candidate): 0.7121 (71.21%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_final_model(model, X_test, y_test):\n",
    "    \"\"\"Évalue le modèle final sur le Test Set\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ÉVALUATION FINALE SUR TEST SET\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Préparation\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_proba = model.predict(dtest, \n",
    "                                  iteration_range=(0, model.best_iteration))\n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "    \n",
    "    # Métriques globales\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', \n",
    "                                zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', \n",
    "                         zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', \n",
    "                          zero_division=0)\n",
    "    \n",
    "    print(\"\\n📊 Performance Metrics:\")\n",
    "    print(f\"   Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision (Avg):    {precision:.4f}\")\n",
    "    print(f\"   Recall (Avg):       {recall:.4f}\")\n",
    "    print(f\"   F1-Score (Macro):   {f1_macro:.4f}\")\n",
    "    print(f\"   F1-Score (Weighted):{f1_weighted:.4f}\")\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['False Positive', 'Candidate', 'Confirmed'],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Performance classe 1 (Candidate)\n",
    "    class1_mask = y_test == 1\n",
    "    if class1_mask.sum() > 0:\n",
    "        class1_acc = accuracy_score(y_test[class1_mask], y_pred[class1_mask])\n",
    "        print(f\"\\n🎯 PERFORMANCE CLASSE 1 (Candidate): \"\n",
    "              f\"{class1_acc:.4f} ({class1_acc*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "metrics = evaluate_final_model(model_final, X_test_enh, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aee742",
   "metadata": {},
   "source": [
    "# VISUALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bde2d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. VISUALISATIONS\n",
      "================================================================================\n",
      "   ✅ Matrice de confusion: confusion_matrix_final.png\n",
      "\n",
      "🏆 TOP 15 FEATURES:\n",
      "              feature  importance\n",
      "        koi_model_snr    7.934336\n",
      "              snr_log    7.769337\n",
      "             koi_prad    6.345869\n",
      "    koi_duration_err1    4.268429\n",
      "          snr_squared    4.063711\n",
      "        koi_prad_err1    3.747423\n",
      "           koi_period    3.699139\n",
      "      koi_period_err1    2.630286\n",
      "        koi_prad_err2    2.568449\n",
      "         koi_duration    2.511777\n",
      "        koi_srad_err1    2.163721\n",
      "       koi_depth_err1    2.036059\n",
      "            koi_depth    1.847937\n",
      "transit_detectability    1.789605\n",
      "             koi_srad    1.777681\n",
      "   ✅ Feature importance: feature_importance_final.png\n"
     ]
    }
   ],
   "source": [
    "def create_visualizations(model, y_test, y_pred):\n",
    "    \"\"\"Crée les visualisations du modèle\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"9. VISUALISATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['False Positive', 'Candidate', 'Confirmed'],\n",
    "                yticklabels=['False Positive', 'Candidate', 'Confirmed'])\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_PATH}/confusion_matrix_final.png', dpi=300)\n",
    "    print(\"   ✅ Matrice de confusion: confusion_matrix_final.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Feature importance\n",
    "    importance_dict = model.get_score(importance_type='gain')\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': list(importance_dict.keys()),\n",
    "        'importance': list(importance_dict.values())\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n🏆 TOP 15 FEATURES:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_20 = feature_importance.head(20)\n",
    "    plt.barh(top_20['feature'], top_20['importance'], color='steelblue')\n",
    "    plt.xlabel('Importance (Gain)')\n",
    "    plt.title('Top 20 Features', fontsize=16, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_PATH}/feature_importance_final.png', dpi=300)\n",
    "    print(\"   ✅ Feature importance: feature_importance_final.png\")\n",
    "    plt.close()\n",
    "\n",
    "create_visualizations(model_final, y_test, metrics['y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2be25",
   "metadata": {},
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f8d1b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "10. SAUVEGARDE\n",
      "================================================================================\n",
      "   ✅ Modèle: exoplanet_xgboost_final.json\n",
      "   ✅ Métadonnées: model_metadata_final.json\n"
     ]
    }
   ],
   "source": [
    "def save_model_and_metadata(model, best_params, metrics, class_weights, \n",
    "                            X_train_val, X_test):\n",
    "    \"\"\"Sauvegarde le modèle et ses métadonnées\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"10. SAUVEGARDE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Sauvegarde du modèle\n",
    "    model.save_model(f'{MODEL_PATH}/exoplanet_xgboost_final.json')\n",
    "    print(\"   ✅ Modèle: exoplanet_xgboost_final.json\")\n",
    "    \n",
    "    # Métadonnées\n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'xgboost_version': xgb.__version__,\n",
    "        'device': device,\n",
    "        'training_strategy': 'Train+Val combined for final model',\n",
    "        'best_hyperparameters': {\n",
    "            'learning_rate': best_params['learning_rate'],\n",
    "            'max_depth': best_params['max_depth'],\n",
    "            'min_child_weight': best_params['min_child_weight'],\n",
    "            'subsample': best_params['subsample'],\n",
    "            'colsample_bytree': best_params['colsample_bytree']\n",
    "        },\n",
    "        'best_iteration': int(model.best_iteration),\n",
    "        'best_score': float(model.best_score),\n",
    "        'test_metrics': {\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'precision': float(metrics['precision']),\n",
    "            'recall': float(metrics['recall']),\n",
    "            'f1_macro': float(metrics['f1_macro']),\n",
    "            'f1_weighted': float(metrics['f1_weighted'])\n",
    "        },\n",
    "        'class_weights': {str(k): float(v) for k, v in class_weights.items()},\n",
    "        'features_engineered': [\n",
    "            'transit_depth_duration_ratio',\n",
    "            'snr_log',\n",
    "            'snr_squared',\n",
    "            'transit_detectability'\n",
    "        ],\n",
    "        'training_samples': int(X_train_val.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0])\n",
    "    }\n",
    "    \n",
    "    with open(f'{MODEL_PATH}/model_metadata_final.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(\"   ✅ Métadonnées: model_metadata_final.json\")\n",
    "\n",
    "save_model_and_metadata(model_final, best_params, metrics, class_weights,\n",
    "                        X_train_val, X_test_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02cef0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved: ./xgboost_model_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save model\n",
    "model_path = f'./xgboost_model_v1.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"✅ Model saved: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

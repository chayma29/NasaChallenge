{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66b1777",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a2bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCRIPT 1: RECHERCHE D'HYPERPARAMÈTRES (GPU - SANS CV)\n",
      "================================================================================\n",
      "\n",
      "1. VÉRIFICATION CUDA\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost version: 2.1.4\n",
      "Support GPU: OUI\n",
      "\n",
      "2. CHARGEMENT DES DONNÉES\n",
      "--------------------------------------------------------------------------------\n",
      "X_train: (6694, 13)\n",
      "X_val: (956, 13)\n",
      "y_train: (6694,)\n",
      "y_val: (956,)\n",
      "\n",
      "Distribution des classes (Train):\n",
      "  0: 3387 (50.6%)\n",
      "  1: 1385 (20.7%)\n",
      "  2: 1922 (28.7%)\n",
      "\n",
      "3. CALCUL DES POIDS DES CLASSES\n",
      "--------------------------------------------------------------------------------\n",
      "Poids calculés pour gérer le déséquilibre\n",
      "\n",
      "4. RECHERCHE D'HYPERPARAMÈTRES (TRAIN → VAL)\n",
      "--------------------------------------------------------------------------------\n",
      "Nombre de combinaisons à tester: 1000\n",
      "Évaluation: Train → Val (pas de cross-validation)\n",
      "\n",
      "Démarrage de la recherche...\n",
      "  [   1/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.7879 (Train = 0.8631)\n",
      "  [   3/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.7937 (Train = 0.8469)\n",
      "  [   4/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.7964 (Train = 0.9024)\n",
      "  [   6/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8030 (Train = 0.9197)\n",
      "  [  50/1000] Val F1 = 0.7852 (Meilleur = 0.8030)\n",
      "  [  76/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8045 (Train = 0.8902)\n",
      "  [ 100/1000] Val F1 = 0.7515 (Meilleur = 0.8045)\n",
      "  [ 107/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8047 (Train = 0.9228)\n",
      "  [ 150/1000] Val F1 = 0.7823 (Meilleur = 0.8047)\n",
      "  [ 200/1000] Val F1 = 0.7947 (Meilleur = 0.8047)\n",
      "  [ 206/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8051 (Train = 0.9428)\n",
      "  [ 228/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8053 (Train = 0.9492)\n",
      "  [ 243/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8062 (Train = 0.9570)\n",
      "  [ 250/1000] Val F1 = 0.7722 (Meilleur = 0.8062)\n",
      "  [ 300/1000] Val F1 = 0.7907 (Meilleur = 0.8062)\n",
      "  [ 350/1000] Val F1 = 0.7812 (Meilleur = 0.8062)\n",
      "  [ 400/1000] Val F1 = 0.7773 (Meilleur = 0.8062)\n",
      "  [ 450/1000] Val F1 = 0.8014 (Meilleur = 0.8062)\n",
      "  [ 500/1000] Val F1 = 0.7786 (Meilleur = 0.8062)\n",
      "  [ 550/1000] Val F1 = 0.8019 (Meilleur = 0.8062)\n",
      "  [ 600/1000] Val F1 = 0.7715 (Meilleur = 0.8062)\n",
      "  [ 650/1000] Val F1 = 0.7808 (Meilleur = 0.8062)\n",
      "  [ 667/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8064 (Train = 0.9618)\n",
      "  [ 700/1000] Val F1 = 0.7760 (Meilleur = 0.8064)\n",
      "  [ 750/1000] Val F1 = 0.7943 (Meilleur = 0.8064)\n",
      "  [ 800/1000] Val F1 = 0.7932 (Meilleur = 0.8064)\n",
      "  [ 850/1000] Val F1 = 0.8003 (Meilleur = 0.8064)\n",
      "  [ 885/1000] ⭐ NOUVEAU MEILLEUR: Val F1 = 0.8088 (Train = 0.9584)\n",
      "  [ 900/1000] Val F1 = 0.8020 (Meilleur = 0.8088)\n",
      "  [ 950/1000] Val F1 = 0.7731 (Meilleur = 0.8088)\n",
      "  [1000/1000] Val F1 = 0.7528 (Meilleur = 0.8088)\n",
      "\n",
      "5. MEILLEURS HYPERPARAMÈTRES TROUVÉS\n",
      "--------------------------------------------------------------------------------\n",
      "Meilleurs hyperparamètres:\n",
      "  colsample_bytree: 0.7\n",
      "  gamma: 0.0\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 11\n",
      "  min_child_weight: 5\n",
      "  n_estimators: 200\n",
      "  reg_alpha: 0.0\n",
      "  reg_lambda: 3.0\n",
      "  subsample: 0.6\n",
      "\n",
      "Meilleur F1 sur validation: 0.8088\n",
      "Temps de recherche: 40.51 minutes\n",
      "\n",
      "Meilleurs paramètres sauvegardés: best_hyperparameters.json\n",
      "Tous les résultats: hyperparameter_search_results.csv\n",
      "\n",
      "6. ANALYSE DES RÉSULTATS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 meilleures combinaisons:\n",
      "\n",
      "Rank 885:\n",
      "  Val F1: 0.8088\n",
      "  Train F1: 0.9584\n",
      "  Overfitting: 0.1496\n",
      "\n",
      "Rank 667:\n",
      "  Val F1: 0.8064\n",
      "  Train F1: 0.9618\n",
      "  Overfitting: 0.1554\n",
      "\n",
      "Rank 243:\n",
      "  Val F1: 0.8062\n",
      "  Train F1: 0.9570\n",
      "  Overfitting: 0.1508\n",
      "\n",
      "Rank 228:\n",
      "  Val F1: 0.8053\n",
      "  Train F1: 0.9492\n",
      "  Overfitting: 0.1439\n",
      "\n",
      "Rank 206:\n",
      "  Val F1: 0.8051\n",
      "  Train F1: 0.9428\n",
      "  Overfitting: 0.1377\n",
      "\n",
      "7. VISUALISATION\n",
      "--------------------------------------------------------------------------------\n",
      "Visualisation sauvegardée: hyperparameter_search_analysis.png\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ - RECHERCHE TERMINÉE\n",
      "================================================================================\n",
      "Temps total: 40.51 minutes\n",
      "Combinaisons testées: 1000\n",
      "Meilleur F1 (Val): 0.8088\n",
      "Cross-validation: NON (split Train/Val)\n",
      "GPU utilisé: OUI\n",
      "\n",
      "Meilleurs hyperparamètres:\n",
      "  colsample_bytree: 0.7\n",
      "  gamma: 0.0\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 11\n",
      "  min_child_weight: 5\n",
      "  n_estimators: 200\n",
      "  reg_alpha: 0.0\n",
      "  reg_lambda: 3.0\n",
      "  subsample: 0.6\n",
      "================================================================================\n",
      "\n",
      "Fichiers générés:\n",
      "  - best_hyperparameters.json\n",
      "  - hyperparameter_search_results.csv\n",
      "  - hyperparameter_search_analysis.png\n",
      "\n",
      "Prochaine étape: Entraîner le modèle final avec ces hyperparamètres\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT 1: RECHERCHE D'HYPERPARAMÈTRES AVEC RANDOMIZED SEARCH (GPU)\n",
    "==================================================================\n",
    "Recherche sur Train, évaluation sur Val (SANS cross-validation)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "BASE_PATH = 'C:/Users/chaym/Desktop/NasaChallenge'\n",
    "DATA_PATH = f'{BASE_PATH}/data/processed'\n",
    "MODEL_PATH = f'{BASE_PATH}/models/xgboost_model'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCRIPT 1: RECHERCHE D'HYPERPARAMÈTRES (GPU - SANS CV)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# VÉRIFICATION CUDA\n",
    "# ====================================================================\n",
    "print(\"\\n1. VÉRIFICATION CUDA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "build_info = xgb.build_info()\n",
    "\n",
    "if not build_info.get('USE_CUDA', False):\n",
    "    print(\"ERREUR: XGBoost n'a pas été compilé avec CUDA\")\n",
    "    exit(1)\n",
    "print(\"Support GPU: OUI\")\n",
    "\n",
    "# ====================================================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ====================================================================\n",
    "print(\"\\n2. CHARGEMENT DES DONNÉES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "X_val = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').values.ravel()\n",
    "y_val = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').values.ravel()\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "\n",
    "print(\"\\nDistribution des classes (Train):\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "for class_name, count in train_dist.items():\n",
    "    print(f\"  {class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# ====================================================================\n",
    "# POIDS DES CLASSES\n",
    "# ====================================================================\n",
    "print(\"\\n3. CALCUL DES POIDS DES CLASSES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(\"Poids calculés pour gérer le déséquilibre\")\n",
    "\n",
    "# ====================================================================\n",
    "# RECHERCHE D'HYPERPARAMÈTRES MANUELLE\n",
    "# ====================================================================\n",
    "print(\"\\n4. RECHERCHE D'HYPERPARAMÈTRES (TRAIN → VAL)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 120, 150, 200],\n",
    "    'max_depth': [5, 7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.05],\n",
    "    'subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [1.0, 2.0, 3.0, 5.0],\n",
    "    'gamma': [0, 0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "n_iter = 1000\n",
    "print(f\"Nombre de combinaisons à tester: {n_iter}\")\n",
    "print(\"Évaluation: Train → Val (pas de cross-validation)\")\n",
    "\n",
    "# Configuration GPU de base\n",
    "base_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': False\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_val_f1 = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "np.random.seed(42)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nDémarrage de la recherche...\")\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Génération aléatoire des hyperparamètres\n",
    "    params = {\n",
    "        'n_estimators': int(np.random.choice(param_dist['n_estimators'])),\n",
    "        'max_depth': int(np.random.choice(param_dist['max_depth'])),\n",
    "        'learning_rate': float(np.random.choice(param_dist['learning_rate'])),\n",
    "        'subsample': float(np.random.choice(param_dist['subsample'])),\n",
    "        'colsample_bytree': float(np.random.choice(param_dist['colsample_bytree'])),\n",
    "        'min_child_weight': int(np.random.choice(param_dist['min_child_weight'])),\n",
    "        'reg_alpha': float(np.random.choice(param_dist['reg_alpha'])),\n",
    "        'reg_lambda': float(np.random.choice(param_dist['reg_lambda'])),\n",
    "        'gamma': float(np.random.choice(param_dist['gamma']))\n",
    "    }\n",
    "    \n",
    "    # Combiner avec les paramètres de base\n",
    "    all_params = {**base_params, **params}\n",
    "    \n",
    "    # Entraînement\n",
    "    model = XGBClassifier(**all_params)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calcul des scores\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Sauvegarde des résultats\n",
    "    results.append({\n",
    "        'iteration': i + 1,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'overfitting': train_f1 - val_f1,\n",
    "        **params\n",
    "    })\n",
    "    \n",
    "    # Mise à jour du meilleur modèle\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_params = params.copy()\n",
    "        best_model = model\n",
    "        print(f\"  [{i+1:4d}/{n_iter}] ⭐ NOUVEAU MEILLEUR: Val F1 = {val_f1:.4f} (Train = {train_f1:.4f})\")\n",
    "    elif (i + 1) % 50 == 0:\n",
    "        print(f\"  [{i+1:4d}/{n_iter}] Val F1 = {val_f1:.4f} (Meilleur = {best_val_f1:.4f})\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# ====================================================================\n",
    "# MEILLEURS RÉSULTATS\n",
    "# ====================================================================\n",
    "print(\"\\n5. MEILLEURS HYPERPARAMÈTRES TROUVÉS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres:\")\n",
    "for k, v in sorted(best_params.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nMeilleur F1 sur validation: {best_val_f1:.4f}\")\n",
    "print(f\"Temps de recherche: {elapsed_time/60:.2f} minutes\")\n",
    "\n",
    "# Sauvegarde JSON\n",
    "config = {\n",
    "    'best_params': best_params,\n",
    "    'best_val_f1': float(best_val_f1),\n",
    "    'search_time_minutes': float(elapsed_time/60),\n",
    "    'n_iter': int(n_iter),\n",
    "    'cross_validation': False,\n",
    "    'gpu_used': True,\n",
    "    'sample_weights_used': True,\n",
    "    'dataset_size': int(len(y_train)),\n",
    "    'n_features': int(X_train.shape[1])\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_PATH}/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "print(\"\\nMeilleurs paramètres sauvegardés: best_hyperparameters.json\")\n",
    "\n",
    "# Sauvegarde de tous les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('val_f1', ascending=False)\n",
    "results_df.to_csv(f'{MODEL_PATH}/hyperparameter_search_results.csv', index=False)\n",
    "print(\"Tous les résultats: hyperparameter_search_results.csv\")\n",
    "\n",
    "# ====================================================================\n",
    "# ANALYSE DES RÉSULTATS\n",
    "# ====================================================================\n",
    "print(\"\\n6. ANALYSE DES RÉSULTATS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nTop 5 meilleures combinaisons:\")\n",
    "for idx, row in results_df.head(5).iterrows():\n",
    "    print(f\"\\nRank {idx+1}:\")\n",
    "    print(f\"  Val F1: {row['val_f1']:.4f}\")\n",
    "    print(f\"  Train F1: {row['train_f1']:.4f}\")\n",
    "    print(f\"  Overfitting: {row['overfitting']:.4f}\")\n",
    "\n",
    "# ====================================================================\n",
    "# VISUALISATION\n",
    "# ====================================================================\n",
    "print(\"\\n7. VISUALISATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Distribution des scores Val\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(results_df['val_f1'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(best_val_f1, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Meilleur: {best_val_f1:.4f}')\n",
    "ax1.set_xlabel('F1 Score (Validation)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fréquence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des Scores Validation', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Top 20 configurations\n",
    "ax2 = axes[0, 1]\n",
    "top_20 = results_df.head(20)\n",
    "ax2.barh(range(len(top_20)), top_20['val_f1'], color='steelblue')\n",
    "ax2.set_xlabel('F1 Score (Validation)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Rang', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Top 20 Configurations', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Train vs Val\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(results_df['train_f1'], results_df['val_f1'], alpha=0.5, s=30)\n",
    "ax3.plot([0.5, 1], [0.5, 1], 'r--', linewidth=2, label='Train = Val')\n",
    "best_row = results_df.iloc[0]\n",
    "ax3.scatter(best_row['train_f1'], best_row['val_f1'], color='red', s=200, \n",
    "            marker='*', label='Meilleur', zorder=5, edgecolor='black', linewidth=2)\n",
    "ax3.set_xlabel('F1 Score (Train)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('F1 Score (Validation)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Train vs Validation', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Overfitting\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(results_df['overfitting'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax4.axvline(best_row['overfitting'], color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Meilleur: {best_row[\"overfitting\"]:.4f}')\n",
    "ax4.set_xlabel('Overfitting (Train - Val)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Fréquence', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Distribution de l\\'Overfitting', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/hyperparameter_search_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualisation sauvegardée: hyperparameter_search_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# RÉSUMÉ\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ - RECHERCHE TERMINÉE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Temps total: {elapsed_time/60:.2f} minutes\")\n",
    "print(f\"Combinaisons testées: {n_iter}\")\n",
    "print(f\"Meilleur F1 (Val): {best_val_f1:.4f}\")\n",
    "print(f\"Cross-validation: NON (split Train/Val)\")\n",
    "print(f\"GPU utilisé: OUI\")\n",
    "print(\"\\nMeilleurs hyperparamètres:\")\n",
    "for k, v in sorted(best_params.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFichiers générés:\")\n",
    "print(\"  - best_hyperparameters.json\")\n",
    "print(\"  - hyperparameter_search_results.csv\")\n",
    "print(\"  - hyperparameter_search_analysis.png\")\n",
    "print(\"\\nProchaine étape: Entraîner le modèle final avec ces hyperparamètres\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2f9fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbf3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCRIPT 2: ENTRAÎNEMENT DU MODÈLE FINAL\n",
      "================================================================================\n",
      "\n",
      "1. VÉRIFICATION CUDA\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost version: 2.1.4\n",
      "Support GPU: OUI\n",
      "\n",
      "2. CHARGEMENT DES DONNÉES\n",
      "--------------------------------------------------------------------------------\n",
      "X_train: (6694, 13)\n",
      "X_val: (956, 13)\n",
      "X_test: (957, 13)\n",
      "\n",
      "X_train_full (Train+Val): (7650, 13)\n",
      "y_train_full (Train+Val): (7650,)\n",
      "\n",
      "Distribution des classes (Train+Val):\n",
      "  Classe 0: 3871 (50.6%)\n",
      "  Classe 1: 1583 (20.7%)\n",
      "  Classe 2: 2196 (28.7%)\n",
      "\n",
      "Distribution des classes (Test):\n",
      "  Classe 0: 484 (50.6%)\n",
      "  Classe 1: 198 (20.7%)\n",
      "  Classe 2: 275 (28.7%)\n",
      "\n",
      "3. CALCUL DES POIDS DES CLASSES\n",
      "--------------------------------------------------------------------------------\n",
      "Poids calculés pour gérer le déséquilibre des classes\n",
      "\n",
      "4. CHARGEMENT DES MEILLEURS HYPERPARAMÈTRES\n",
      "--------------------------------------------------------------------------------\n",
      "Hyperparamètres utilisés:\n",
      "  colsample_bytree: 0.7\n",
      "  gamma: 0.0\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 11\n",
      "  min_child_weight: 5\n",
      "  n_estimators: 200\n",
      "  reg_alpha: 0.0\n",
      "  reg_lambda: 3.0\n",
      "  subsample: 0.6\n",
      "\n",
      "5. ENTRAÎNEMENT DU MODÈLE FINAL\n",
      "--------------------------------------------------------------------------------\n",
      "Entraînement sur Train+Val...\n",
      "  Taille: 7650 exemples\n",
      "  Features: 13\n",
      "\n",
      "Entraînement terminé en 5.10 secondes\n",
      "\n",
      "6. ÉVALUATION SUR LE SET DE TEST\n",
      "--------------------------------------------------------------------------------\n",
      "RÉSULTATS SUR TEST:\n",
      "  Accuracy: 0.7712\n",
      "  Precision: 0.7808\n",
      "  Recall: 0.7712\n",
      "  F1 Score: 0.7747\n",
      "\n",
      "RÉSULTATS SUR TRAIN+VAL:\n",
      "  Accuracy: 0.9571\n",
      "  F1 Score: 0.9575\n",
      "\n",
      "Overfitting: 0.1828\n",
      "\n",
      "7. RAPPORT DE CLASSIFICATION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Rapport détaillé par classe:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.86      0.79      0.82       484\n",
      "    Classe 1       0.53      0.61      0.57       198\n",
      "    Classe 2       0.82      0.85      0.84       275\n",
      "\n",
      "    accuracy                           0.77       957\n",
      "   macro avg       0.74      0.75      0.74       957\n",
      "weighted avg       0.78      0.77      0.77       957\n",
      "\n",
      "\n",
      "8. MATRICE DE CONFUSION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Matrice de confusion:\n",
      "[[383  76  25]\n",
      " [ 52 120  26]\n",
      " [ 11  29 235]]\n",
      "\n",
      "Matrice de confusion sauvegardée: confusion_matrix_test.png\n",
      "\n",
      "9. IMPORTANCE DES FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 15 features les plus importantes:\n",
      "  koi_prad: 0.1516\n",
      "  koi_model_snr: 0.1332\n",
      "  koi_duration_err1: 0.1133\n",
      "  koi_period: 0.0907\n",
      "  koi_prad_err1: 0.0798\n",
      "  koi_period_err1: 0.0671\n",
      "  koi_duration: 0.0597\n",
      "  koi_depth: 0.0540\n",
      "  koi_prad_err2: 0.0539\n",
      "  koi_depth_err1: 0.0528\n",
      "  koi_srad_err1: 0.0527\n",
      "  koi_srad: 0.0459\n",
      "  koi_srad_err2: 0.0455\n",
      "\n",
      "Importance des features sauvegardée: feature_importance.png\n",
      "\n",
      "10. SAUVEGARDE DU MODÈLE FINAL\n",
      "--------------------------------------------------------------------------------\n",
      "Modèle sauvegardé: xgboost_final_model.pkl\n",
      "Résultats finaux sauvegardés: final_model_results.json\n",
      "\n",
      "11. VISUALISATIONS COMPARATIVES\n",
      "--------------------------------------------------------------------------------\n",
      "Visualisations comparatives sauvegardées: final_model_comparison.png\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ FINAL - MODÈLE ENTRAÎNÉ ET ÉVALUÉ\n",
      "================================================================================\n",
      "\n",
      "Données d'entraînement: Train + Val (7650 exemples)\n",
      "Données de test: Test (957 exemples)\n",
      "Nombre de features: 13\n",
      "\n",
      "Hyperparamètres utilisés:\n",
      "  colsample_bytree: 0.7\n",
      "  gamma: 0.0\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 11\n",
      "  min_child_weight: 5\n",
      "  n_estimators: 200\n",
      "  reg_alpha: 0.0\n",
      "  reg_lambda: 3.0\n",
      "  subsample: 0.6\n",
      "\n",
      "PERFORMANCES SUR TEST:\n",
      "  Accuracy:  0.7712\n",
      "  Precision: 0.7808\n",
      "  Recall:    0.7712\n",
      "  F1 Score:  0.7747\n",
      "\n",
      "Temps d'entraînement: 5.10 secondes\n",
      "GPU utilisé: OUI\n",
      "Sample weights: OUI\n",
      "\n",
      "================================================================================\n",
      "FICHIERS GÉNÉRÉS:\n",
      "================================================================================\n",
      "  1. xgboost_final_model.pkl - Modèle entraîné\n",
      "  2. final_model_results.json - Résultats détaillés\n",
      "  3. confusion_matrix_test.png - Matrice de confusion\n",
      "  4. feature_importance.png - Importance des features\n",
      "  5. feature_importance.csv - Importance (CSV)\n",
      "  6. final_model_comparison.png - Comparaisons\n",
      "================================================================================\n",
      "\n",
      "✅ ENTRAÎNEMENT ET ÉVALUATION TERMINÉS AVEC SUCCÈS!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT 2: ENTRAÎNEMENT DU MODÈLE FINAL AVEC MEILLEURS HYPERPARAMÈTRES\n",
    "=====================================================================\n",
    "Entraîne sur Train+Val, évalue sur Test\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (f1_score, classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_score, recall_score)\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "BASE_PATH = 'C:/Users/chaym/Desktop/NasaChallenge'\n",
    "DATA_PATH = f'{BASE_PATH}/data/processed'\n",
    "MODEL_PATH = f'{BASE_PATH}/models/xgboost_model'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCRIPT 2: ENTRAÎNEMENT DU MODÈLE FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# VÉRIFICATION CUDA\n",
    "# ====================================================================\n",
    "print(\"\\n1. VÉRIFICATION CUDA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "build_info = xgb.build_info()\n",
    "\n",
    "if not build_info.get('USE_CUDA', False):\n",
    "    print(\"ERREUR: XGBoost n'a pas été compilé avec CUDA\")\n",
    "    exit(1)\n",
    "print(\"Support GPU: OUI\")\n",
    "\n",
    "# ====================================================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ====================================================================\n",
    "print(\"\\n2. CHARGEMENT DES DONNÉES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "X_val = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "X_test = pd.read_csv(f'{DATA_PATH}/step6_X_test.csv')\n",
    "\n",
    "y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').values.ravel()\n",
    "y_val = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').values.ravel()\n",
    "y_test = pd.read_csv(f'{DATA_PATH}/step6_y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "# Combiner Train + Val pour l'entraînement final\n",
    "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"\\nX_train_full (Train+Val): {X_train_full.shape}\")\n",
    "print(f\"y_train_full (Train+Val): {y_train_full.shape}\")\n",
    "\n",
    "print(\"\\nDistribution des classes (Train+Val):\")\n",
    "train_full_dist = pd.Series(y_train_full).value_counts().sort_index()\n",
    "for class_name, count in train_full_dist.items():\n",
    "    print(f\"  Classe {class_name}: {count} ({count/len(y_train_full)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDistribution des classes (Test):\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "for class_name, count in test_dist.items():\n",
    "    print(f\"  Classe {class_name}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# ====================================================================\n",
    "# CALCUL DES POIDS\n",
    "# ====================================================================\n",
    "print(\"\\n3. CALCUL DES POIDS DES CLASSES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_full)\n",
    "print(\"Poids calculés pour gérer le déséquilibre des classes\")\n",
    "\n",
    "# ====================================================================\n",
    "# MEILLEURS HYPERPARAMÈTRES\n",
    "# ====================================================================\n",
    "print(\"\\n4. CHARGEMENT DES MEILLEURS HYPERPARAMÈTRES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.7,\n",
    "    'gamma': 0.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 5,\n",
    "    'n_estimators': 200,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 3.0,\n",
    "    'subsample': 0.6\n",
    "}\n",
    "\n",
    "print(\"Hyperparamètres utilisés:\")\n",
    "for k, v in sorted(best_params.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ====================================================================\n",
    "# ENTRAÎNEMENT DU MODÈLE FINAL\n",
    "# ====================================================================\n",
    "print(\"\\n5. ENTRAÎNEMENT DU MODÈLE FINAL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Configuration complète\n",
    "final_model = XGBClassifier(\n",
    "    # Meilleurs hyperparamètres\n",
    "    **best_params,\n",
    "    \n",
    "    # Paramètres de base\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    \n",
    "    # GPU\n",
    "    tree_method='gpu_hist',\n",
    "    gpu_id=0,\n",
    "    predictor='gpu_predictor',\n",
    "    enable_categorical=False\n",
    ")\n",
    "\n",
    "print(\"Entraînement sur Train+Val...\")\n",
    "print(f\"  Taille: {X_train_full.shape[0]} exemples\")\n",
    "print(f\"  Features: {X_train_full.shape[1]}\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "final_model.fit(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    sample_weight=sample_weights,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nEntraînement terminé en {training_time:.2f} secondes\")\n",
    "\n",
    "# ====================================================================\n",
    "# ÉVALUATION SUR TEST\n",
    "# ====================================================================\n",
    "print(\"\\n6. ÉVALUATION SUR LE SET DE TEST\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Prédictions\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_train_full_pred = final_model.predict(X_train_full)\n",
    "\n",
    "# Métriques\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "train_full_f1 = f1_score(y_train_full, y_train_full_pred, average='weighted')\n",
    "train_full_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "\n",
    "print(\"RÉSULTATS SUR TEST:\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nRÉSULTATS SUR TRAIN+VAL:\")\n",
    "print(f\"  Accuracy: {train_full_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {train_full_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nOverfitting: {train_full_f1 - test_f1:.4f}\")\n",
    "\n",
    "# ====================================================================\n",
    "# RAPPORT DE CLASSIFICATION DÉTAILLÉ\n",
    "# ====================================================================\n",
    "print(\"\\n7. RAPPORT DE CLASSIFICATION (TEST)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nRapport détaillé par classe:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Classe 0', 'Classe 1', 'Classe 2']))\n",
    "\n",
    "# ====================================================================\n",
    "# MATRICE DE CONFUSION\n",
    "# ====================================================================\n",
    "print(\"\\n8. MATRICE DE CONFUSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'],\n",
    "            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\n",
    "plt.title('Matrice de Confusion - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Classe Prédite', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/confusion_matrix_test.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nMatrice de confusion sauvegardée: confusion_matrix_test.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# ====================================================================\n",
    "print(\"\\n9. IMPORTANCE DES FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Obtenir l'importance des features\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Créer un DataFrame pour l'importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features les plus importantes:\")\n",
    "for idx, row in importance_df.head(15).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features les Plus Importantes', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nImportance des features sauvegardée: feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "# Sauvegarder l'importance dans un CSV\n",
    "importance_df.to_csv(f'{MODEL_PATH}/feature_importance.csv', index=False)\n",
    "\n",
    "# ====================================================================\n",
    "# SAUVEGARDE DU MODÈLE\n",
    "# ====================================================================\n",
    "print(\"\\n10. SAUVEGARDE DU MODÈLE FINAL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "joblib.dump(final_model, f'{MODEL_PATH}/xgboost_final_model.pkl')\n",
    "print(\"Modèle sauvegardé: xgboost_final_model.pkl\")\n",
    "\n",
    "# Sauvegarder les résultats finaux\n",
    "final_results = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'training_data': 'Train + Val',\n",
    "    'test_data': 'Test',\n",
    "    'n_train_samples': int(len(y_train_full)),\n",
    "    'n_test_samples': int(len(y_test)),\n",
    "    'n_features': int(X_train_full.shape[1]),\n",
    "    \n",
    "    'hyperparameters': best_params,\n",
    "    \n",
    "    'test_metrics': {\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'precision': float(test_precision),\n",
    "        'recall': float(test_recall),\n",
    "        'f1_score': float(test_f1)\n",
    "    },\n",
    "    \n",
    "    'train_metrics': {\n",
    "        'accuracy': float(train_full_accuracy),\n",
    "        'f1_score': float(train_full_f1)\n",
    "    },\n",
    "    \n",
    "    'overfitting': float(train_full_f1 - test_f1),\n",
    "    'training_time_seconds': float(training_time),\n",
    "    'gpu_used': True,\n",
    "    'sample_weights_used': True\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_PATH}/final_model_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(\"Résultats finaux sauvegardés: final_model_results.json\")\n",
    "\n",
    "# ====================================================================\n",
    "# VISUALISATION COMPARATIVE\n",
    "# ====================================================================\n",
    "print(\"\\n11. VISUALISATIONS COMPARATIVES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Comparaison des métriques Train vs Test\n",
    "ax1 = axes[0]\n",
    "metrics = ['Accuracy', 'F1 Score']\n",
    "train_scores = [train_full_accuracy, train_full_f1]\n",
    "test_scores = [test_accuracy, test_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, train_scores, width, label='Train+Val', color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, test_scores, width, label='Test', color='coral')\n",
    "\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Comparaison Train+Val vs Test', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Distribution des prédictions par classe\n",
    "ax2 = axes[1]\n",
    "pred_dist = pd.Series(y_test_pred).value_counts().sort_index()\n",
    "true_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "x = np.arange(len(pred_dist))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, true_dist.values, width, label='Vraies classes', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, pred_dist.values, width, label='Prédictions', color='coral')\n",
    "\n",
    "ax2.set_ylabel('Nombre d\\'exemples', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Distribution des Classes (Test)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'Classe {i}' for i in range(len(pred_dist))])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/final_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualisations comparatives sauvegardées: final_model_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# RÉSUMÉ FINAL\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ FINAL - MODÈLE ENTRAÎNÉ ET ÉVALUÉ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDonnées d'entraînement: Train + Val ({len(y_train_full)} exemples)\")\n",
    "print(f\"Données de test: Test ({len(y_test)} exemples)\")\n",
    "print(f\"Nombre de features: {X_train_full.shape[1]}\")\n",
    "\n",
    "print(\"\\nHyperparamètres utilisés:\")\n",
    "for k, v in sorted(best_params.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR TEST:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nTemps d'entraînement: {training_time:.2f} secondes\")\n",
    "print(f\"GPU utilisé: OUI\")\n",
    "print(f\"Sample weights: OUI\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FICHIERS GÉNÉRÉS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  1. xgboost_final_model.pkl - Modèle entraîné\")\n",
    "print(\"  2. final_model_results.json - Résultats détaillés\")\n",
    "print(\"  3. confusion_matrix_test.png - Matrice de confusion\")\n",
    "print(\"  4. feature_importance.png - Importance des features\")\n",
    "print(\"  5. feature_importance.csv - Importance (CSV)\")\n",
    "print(\"  6. final_model_comparison.png - Comparaisons\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ ENTRAÎNEMENT ET ÉVALUATION TERMINÉS AVEC SUCCÈS!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7eb153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCRIPT 3: AMÉLIORATION CLASSE 1 - AJUSTEMENT DES POIDS\n",
      "================================================================================\n",
      "\n",
      "1. VÉRIFICATION CUDA\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost version: 2.1.4\n",
      "Support GPU: OUI\n",
      "\n",
      "2. CHARGEMENT DES DONNÉES\n",
      "--------------------------------------------------------------------------------\n",
      "X_train_full: (7650, 13)\n",
      "X_test: (957, 13)\n",
      "\n",
      "Distribution des classes (Train+Val):\n",
      "  Classe 0: 3871 (50.6%)\n",
      "  Classe 1: 1583 (20.7%)\n",
      "  Classe 2: 2196 (28.7%)\n",
      "\n",
      "3. HYPERPARAMÈTRES DE BASE\n",
      "--------------------------------------------------------------------------------\n",
      "Hyperparamètres:\n",
      "  colsample_bytree: 0.7\n",
      "  enable_categorical: False\n",
      "  eval_metric: mlogloss\n",
      "  gamma: 0.0\n",
      "  gpu_id: 0\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 11\n",
      "  min_child_weight: 5\n",
      "  n_estimators: 200\n",
      "  num_class: 3\n",
      "  objective: multi:softmax\n",
      "  predictor: gpu_predictor\n",
      "  random_state: 42\n",
      "  reg_alpha: 0.0\n",
      "  reg_lambda: 3.0\n",
      "  subsample: 0.6\n",
      "  tree_method: gpu_hist\n",
      "\n",
      "4. BASELINE - POIDS BALANCED STANDARD\n",
      "--------------------------------------------------------------------------------\n",
      "Poids de classes (balanced):\n",
      "  Classe 0: 0.6587\n",
      "  Classe 1: 1.6109\n",
      "  Classe 2: 1.1612\n",
      "\n",
      "Entraînement modèle baseline...\n",
      "\n",
      "RÉSULTATS BASELINE:\n",
      "  F1 global (Test): 0.7747\n",
      "  F1 Classe 1: 0.5674\n",
      "  Precision Classe 1: 0.5333\n",
      "  Recall Classe 1: 0.6061\n",
      "\n",
      "5. TEST DE DIFFÉRENTS MULTIPLICATEURS POUR CLASSE 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test des multiplicateurs...\n",
      "--------------------------------------------------------------------------------\n",
      "Mult 1.0x: F1=0.7747 | F1_C1=0.5674 (P=0.5333, R=0.6061)\n",
      "Mult 1.2x: F1=0.7619 | F1_C1=0.5471 (P=0.5021, R=0.6010)\n",
      "Mult 1.5x: F1=0.7685 | F1_C1=0.5702 (P=0.5100, R=0.6465)\n",
      "Mult 1.8x: F1=0.7675 | F1_C1=0.5689 (P=0.5019, R=0.6566)\n",
      "Mult 2.0x: F1=0.7696 | F1_C1=0.5813 (P=0.5095, R=0.6768)\n",
      "Mult 2.5x: F1=0.7643 | F1_C1=0.5696 (P=0.4891, R=0.6818)\n",
      "Mult 3.0x: F1=0.7678 | F1_C1=0.5821 (P=0.4947, R=0.7071)\n",
      "Mult 3.5x: F1=0.7587 | F1_C1=0.5765 (P=0.4754, R=0.7323)\n",
      "Mult 4.0x: F1=0.7706 | F1_C1=0.5963 (P=0.4983, R=0.7424)\n",
      "\n",
      "6. ANALYSE DES RÉSULTATS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MEILLEURS MULTIPLICATEURS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Meilleur F1 Global:\n",
      "   Multiplicateur: 1.0x\n",
      "   F1 Global: 0.7747\n",
      "   F1 Classe 1: 0.5674\n",
      "\n",
      "2. Meilleur F1 Classe 1:\n",
      "   Multiplicateur: 4.0x\n",
      "   F1 Global: 0.7706\n",
      "   F1 Classe 1: 0.5963\n",
      "\n",
      "3. Meilleur Équilibre (F1 Global + F1 Classe 1):\n",
      "   Multiplicateur: 4.0x\n",
      "   F1 Global: 0.7706\n",
      "   F1 Classe 1: 0.5963\n",
      "\n",
      "AMÉLIORATIONS:\n",
      "  F1 Global: +0.0000\n",
      "  F1 Classe 1: +0.0290\n",
      "\n",
      "7. VISUALISATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Visualisations sauvegardées: class1_weight_optimization.png\n",
      "\n",
      "8. MATRICES DE CONFUSION\n",
      "--------------------------------------------------------------------------------\n",
      "Matrices de confusion sauvegardées: confusion_matrix_comparison.png\n",
      "\n",
      "9. ENTRAÎNEMENT DU MODÈLE FINAL OPTIMISÉ\n",
      "--------------------------------------------------------------------------------\n",
      "Multiplicateur optimal: 4.0x\n",
      "\n",
      "Poids finaux des classes:\n",
      "  Classe 0: 0.6587\n",
      "  Classe 1: 6.4435\n",
      "  Classe 2: 1.1612\n",
      "\n",
      "Entraînement du modèle final...\n",
      "\n",
      "RAPPORT DE CLASSIFICATION FINAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.89      0.74      0.81       484\n",
      "    Classe 1       0.50      0.74      0.60       198\n",
      "    Classe 2       0.86      0.80      0.83       275\n",
      "\n",
      "    accuracy                           0.76       957\n",
      "   macro avg       0.75      0.76      0.74       957\n",
      "weighted avg       0.80      0.76      0.77       957\n",
      "\n",
      "\n",
      "10. SAUVEGARDE DES RÉSULTATS\n",
      "--------------------------------------------------------------------------------\n",
      "Résultats sauvegardés: class1_optimization_results.csv\n",
      "Configuration optimale: class1_optimization_config.json\n",
      "Modèle optimisé: xgboost_optimized_class1.pkl\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ - OPTIMISATION CLASSE 1\n",
      "================================================================================\n",
      "\n",
      "MULTIPLICATEUR OPTIMAL: 4.0x\n",
      "\n",
      "BASELINE:\n",
      "  F1 Global: 0.7747\n",
      "  F1 Classe 1: 0.5674\n",
      "\n",
      "OPTIMISÉ:\n",
      "  F1 Global: 0.7706\n",
      "  F1 Classe 1: 0.5963\n",
      "\n",
      "AMÉLIORATIONS:\n",
      "  F1 Global: +0.0000 (+0.0%)\n",
      "  F1 Classe 1: +0.0290 (+5.1%)\n",
      "\n",
      "================================================================================\n",
      "FICHIERS GÉNÉRÉS:\n",
      "================================================================================\n",
      "  1. class1_optimization_results.csv - Tous les résultats\n",
      "  2. class1_optimization_config.json - Configuration optimale\n",
      "  3. class1_weight_optimization.png - Visualisations\n",
      "  4. confusion_matrix_comparison.png - Comparaison matrices\n",
      "  5. xgboost_optimized_class1.pkl - Modèle optimisé\n",
      "================================================================================\n",
      "\n",
      "✅ OPTIMISATION CLASSE 1 TERMINÉE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT 3: AJUSTEMENT DES POIDS\n",
    "=======================================================\n",
    "Teste différents poids pour améliorer la performance de la Classe 1\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (f1_score, classification_report, confusion_matrix,\n",
    "                             precision_score, recall_score, accuracy_score)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "BASE_PATH = 'C:/Users/chaym/Desktop/NasaChallenge'\n",
    "DATA_PATH = f'{BASE_PATH}/data/processed'\n",
    "MODEL_PATH = f'{BASE_PATH}/models/xgboost_model'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCRIPT 3: AMÉLIORATION CLASSE 1 - AJUSTEMENT DES POIDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# VÉRIFICATION CUDA\n",
    "# ====================================================================\n",
    "print(\"\\n1. VÉRIFICATION CUDA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "build_info = xgb.build_info()\n",
    "\n",
    "if not build_info.get('USE_CUDA', False):\n",
    "    print(\"ERREUR: XGBoost n'a pas été compilé avec CUDA\")\n",
    "    exit(1)\n",
    "print(\"Support GPU: OUI\")\n",
    "\n",
    "# ====================================================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ====================================================================\n",
    "print(\"\\n2. CHARGEMENT DES DONNÉES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "X_val = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "X_test = pd.read_csv(f'{DATA_PATH}/step6_X_test.csv')\n",
    "\n",
    "y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').values.ravel()\n",
    "y_val = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').values.ravel()\n",
    "y_test = pd.read_csv(f'{DATA_PATH}/step6_y_test.csv').values.ravel()\n",
    "\n",
    "# Combiner Train + Val\n",
    "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"X_train_full: {X_train_full.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nDistribution des classes (Train+Val):\")\n",
    "for class_name, count in pd.Series(y_train_full).value_counts().sort_index().items():\n",
    "    print(f\"  Classe {class_name}: {count} ({count/len(y_train_full)*100:.1f}%)\")\n",
    "\n",
    "# ====================================================================\n",
    "# MEILLEURS HYPERPARAMÈTRES (BASELINE)\n",
    "# ====================================================================\n",
    "print(\"\\n3. HYPERPARAMÈTRES DE BASE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.7,\n",
    "    'gamma': 0.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 5,\n",
    "    'n_estimators': 200,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 3.0,\n",
    "    'subsample': 0.6,\n",
    "    \n",
    "    # Paramètres de base\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': False\n",
    "}\n",
    "\n",
    "print(\"Hyperparamètres:\")\n",
    "for k, v in sorted(best_params.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ====================================================================\n",
    "# BASELINE - POIDS BALANCED STANDARD\n",
    "# ====================================================================\n",
    "print(\"\\n4. BASELINE - POIDS BALANCED STANDARD\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calcul des poids standard\n",
    "class_weights_base = compute_class_weight('balanced', \n",
    "                                          classes=np.unique(y_train_full), \n",
    "                                          y=y_train_full)\n",
    "sample_weights_base = np.array([class_weights_base[y] for y in y_train_full])\n",
    "\n",
    "print(\"Poids de classes (balanced):\")\n",
    "for i, w in enumerate(class_weights_base):\n",
    "    print(f\"  Classe {i}: {w:.4f}\")\n",
    "\n",
    "# Entraîner modèle baseline\n",
    "print(\"\\nEntraînement modèle baseline...\")\n",
    "baseline_model = XGBClassifier(**best_params)\n",
    "baseline_model.fit(X_train_full, y_train_full, \n",
    "                   sample_weight=sample_weights_base, verbose=False)\n",
    "\n",
    "# Évaluation baseline\n",
    "y_test_pred_base = baseline_model.predict(X_test)\n",
    "baseline_f1_overall = f1_score(y_test, y_test_pred_base, average='weighted')\n",
    "baseline_f1_class1 = f1_score(y_test, y_test_pred_base, average=None)[1]\n",
    "baseline_precision_class1 = precision_score(y_test, y_test_pred_base, average=None)[1]\n",
    "baseline_recall_class1 = recall_score(y_test, y_test_pred_base, average=None)[1]\n",
    "\n",
    "print(\"\\nRÉSULTATS BASELINE:\")\n",
    "print(f\"  F1 global (Test): {baseline_f1_overall:.4f}\")\n",
    "print(f\"  F1 Classe 1: {baseline_f1_class1:.4f}\")\n",
    "print(f\"  Precision Classe 1: {baseline_precision_class1:.4f}\")\n",
    "print(f\"  Recall Classe 1: {baseline_recall_class1:.4f}\")\n",
    "\n",
    "# ====================================================================\n",
    "# TEST DE DIFFÉRENTS MULTIPLICATEURS POUR CLASSE 1\n",
    "# ====================================================================\n",
    "print(\"\\n5. TEST DE DIFFÉRENTS MULTIPLICATEURS POUR CLASSE 1\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Multiplicateurs à tester\n",
    "multipliers = [1.0, 1.2, 1.5, 1.8, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nTest des multiplicateurs...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for mult in multipliers:\n",
    "    # Calculer les nouveaux poids\n",
    "    class_weights = class_weights_base.copy()\n",
    "    class_weights[1] *= mult  # Multiplier le poids de Classe 1\n",
    "    \n",
    "    sample_weights = np.array([class_weights[y] for y in y_train_full])\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    model = XGBClassifier(**best_params)\n",
    "    model.fit(X_train_full, y_train_full, \n",
    "              sample_weight=sample_weights, verbose=False)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métriques globales\n",
    "    f1_overall = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Métriques par classe\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "    precision_per_class = precision_score(y_test, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    result = {\n",
    "        'multiplier': mult,\n",
    "        'f1_overall': f1_overall,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_class0': f1_per_class[0],\n",
    "        'f1_class1': f1_per_class[1],\n",
    "        'f1_class2': f1_per_class[2],\n",
    "        'precision_class1': precision_per_class[1],\n",
    "        'recall_class1': recall_per_class[1],\n",
    "        'confusion_matrix': cm,\n",
    "        'class_weights': class_weights.copy()\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Afficher\n",
    "    print(f\"Mult {mult:.1f}x: F1={f1_overall:.4f} | \"\n",
    "          f\"F1_C1={f1_per_class[1]:.4f} (P={precision_per_class[1]:.4f}, R={recall_per_class[1]:.4f})\")\n",
    "\n",
    "# ====================================================================\n",
    "# ANALYSE DES RÉSULTATS\n",
    "# ====================================================================\n",
    "print(\"\\n6. ANALYSE DES RÉSULTATS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trouver le meilleur multiplicateur selon différents critères\n",
    "best_overall_idx = results_df['f1_overall'].idxmax()\n",
    "best_class1_idx = results_df['f1_class1'].idxmax()\n",
    "best_balanced_idx = (results_df['f1_overall'] + results_df['f1_class1']).idxmax()\n",
    "\n",
    "print(\"\\nMEILLEURS MULTIPLICATEURS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n1. Meilleur F1 Global:\")\n",
    "print(f\"   Multiplicateur: {results_df.loc[best_overall_idx, 'multiplier']:.1f}x\")\n",
    "print(f\"   F1 Global: {results_df.loc[best_overall_idx, 'f1_overall']:.4f}\")\n",
    "print(f\"   F1 Classe 1: {results_df.loc[best_overall_idx, 'f1_class1']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Meilleur F1 Classe 1:\")\n",
    "print(f\"   Multiplicateur: {results_df.loc[best_class1_idx, 'multiplier']:.1f}x\")\n",
    "print(f\"   F1 Global: {results_df.loc[best_class1_idx, 'f1_overall']:.4f}\")\n",
    "print(f\"   F1 Classe 1: {results_df.loc[best_class1_idx, 'f1_class1']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Meilleur Équilibre (F1 Global + F1 Classe 1):\")\n",
    "print(f\"   Multiplicateur: {results_df.loc[best_balanced_idx, 'multiplier']:.1f}x\")\n",
    "print(f\"   F1 Global: {results_df.loc[best_balanced_idx, 'f1_overall']:.4f}\")\n",
    "print(f\"   F1 Classe 1: {results_df.loc[best_balanced_idx, 'f1_class1']:.4f}\")\n",
    "\n",
    "# Amélioration par rapport au baseline\n",
    "improvement_overall = results_df.loc[best_overall_idx, 'f1_overall'] - baseline_f1_overall\n",
    "improvement_class1 = results_df.loc[best_class1_idx, 'f1_class1'] - baseline_f1_class1\n",
    "\n",
    "print(f\"\\nAMÉLIORATIONS:\")\n",
    "print(f\"  F1 Global: +{improvement_overall:.4f}\")\n",
    "print(f\"  F1 Classe 1: +{improvement_class1:.4f}\")\n",
    "\n",
    "# ====================================================================\n",
    "# VISUALISATIONS\n",
    "# ====================================================================\n",
    "print(\"\\n7. VISUALISATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. F1 Score vs Multiplicateur\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(results_df['multiplier'], results_df['f1_overall'], \n",
    "         marker='o', linewidth=2, markersize=8, label='F1 Global', color='steelblue')\n",
    "ax1.plot(results_df['multiplier'], results_df['f1_class1'], \n",
    "         marker='s', linewidth=2, markersize=8, label='F1 Classe 1', color='coral')\n",
    "ax1.axhline(baseline_f1_overall, linestyle='--', color='steelblue', \n",
    "            alpha=0.5, label='Baseline Global')\n",
    "ax1.axhline(baseline_f1_class1, linestyle='--', color='coral', \n",
    "            alpha=0.5, label='Baseline Classe 1')\n",
    "ax1.set_xlabel('Multiplicateur Classe 1', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Score vs Multiplicateur', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision vs Recall Classe 1\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(results_df['recall_class1'], results_df['precision_class1'], \n",
    "                      c=results_df['multiplier'], s=200, cmap='viridis', \n",
    "                      edgecolors='black', linewidth=2)\n",
    "ax2.scatter(baseline_recall_class1, baseline_precision_class1, \n",
    "            s=300, marker='*', color='red', edgecolors='black', \n",
    "            linewidth=2, label='Baseline', zorder=5)\n",
    "ax2.set_xlabel('Recall Classe 1', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Precision Classe 1', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Precision vs Recall - Classe 1', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Multiplicateur', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 3. F1 par classe\n",
    "ax3 = axes[1, 0]\n",
    "best_mult = results_df.loc[best_balanced_idx, 'multiplier']\n",
    "best_result = results_df.loc[best_balanced_idx]\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "baseline_f1s = [f1_score(y_test, y_test_pred_base, average=None)[i] for i in range(3)]\n",
    "best_f1s = [best_result['f1_class0'], best_result['f1_class1'], best_result['f1_class2']]\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, baseline_f1s, width, label='Baseline', color='lightgray')\n",
    "bars2 = ax3.bar(x + width/2, best_f1s, width, label=f'Mult {best_mult:.1f}x', color='coral')\n",
    "\n",
    "ax3.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('F1 par Classe - Comparaison', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Classe 0', 'Classe 1', 'Classe 2'])\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Tableau récapitulatif\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for idx, row in results_df.iterrows():\n",
    "    table_data.append([\n",
    "        f\"{row['multiplier']:.1f}x\",\n",
    "        f\"{row['f1_overall']:.4f}\",\n",
    "        f\"{row['f1_class1']:.4f}\",\n",
    "        f\"{row['precision_class1']:.4f}\",\n",
    "        f\"{row['recall_class1']:.4f}\"\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                  colLabels=['Mult', 'F1 Global', 'F1 C1', 'Prec C1', 'Rec C1'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Colorer la meilleure ligne\n",
    "for i in range(len(table_data[0])):\n",
    "    table[(best_balanced_idx+1, i)].set_facecolor('#90EE90')\n",
    "    table[(best_balanced_idx+1, i)].set_text_props(weight='bold')\n",
    "\n",
    "ax4.set_title('Résultats Détaillés', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/class1_weight_optimization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualisations sauvegardées: class1_weight_optimization.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# MATRICE DE CONFUSION - COMPARAISON\n",
    "# ====================================================================\n",
    "print(\"\\n8. MATRICES DE CONFUSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Baseline\n",
    "ax1 = axes[0]\n",
    "cm_base = confusion_matrix(y_test, y_test_pred_base)\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=True,\n",
    "            xticklabels=['C0', 'C1', 'C2'],\n",
    "            yticklabels=['C0', 'C1', 'C2'])\n",
    "ax1.set_title(f'Baseline (Mult 1.0x)\\nF1 C1 = {baseline_f1_class1:.4f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Classe Prédite', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Meilleur multiplicateur\n",
    "ax2 = axes[1]\n",
    "best_cm = results_df.loc[best_balanced_idx, 'confusion_matrix']\n",
    "best_mult = results_df.loc[best_balanced_idx, 'multiplier']\n",
    "best_f1_c1 = results_df.loc[best_balanced_idx, 'f1_class1']\n",
    "\n",
    "sns.heatmap(best_cm, annot=True, fmt='d', cmap='Greens', ax=ax2, cbar=True,\n",
    "            xticklabels=['C0', 'C1', 'C2'],\n",
    "            yticklabels=['C0', 'C1', 'C2'])\n",
    "ax2.set_title(f'Optimisé (Mult {best_mult:.1f}x)\\nF1 C1 = {best_f1_c1:.4f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Classe Prédite', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/confusion_matrix_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Matrices de confusion sauvegardées: confusion_matrix_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# ENTRAÎNEMENT DU MODÈLE FINAL OPTIMISÉ\n",
    "# ====================================================================\n",
    "print(\"\\n9. ENTRAÎNEMENT DU MODÈLE FINAL OPTIMISÉ\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_multiplier = results_df.loc[best_balanced_idx, 'multiplier']\n",
    "print(f\"Multiplicateur optimal: {best_multiplier:.1f}x\")\n",
    "\n",
    "# Calculer les poids finaux\n",
    "final_class_weights = class_weights_base.copy()\n",
    "final_class_weights[1] *= best_multiplier\n",
    "final_sample_weights = np.array([final_class_weights[y] for y in y_train_full])\n",
    "\n",
    "print(\"\\nPoids finaux des classes:\")\n",
    "for i, w in enumerate(final_class_weights):\n",
    "    print(f\"  Classe {i}: {w:.4f}\")\n",
    "\n",
    "# Entraîner le modèle final\n",
    "print(\"\\nEntraînement du modèle final...\")\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train_full, y_train_full, \n",
    "                sample_weight=final_sample_weights, verbose=False)\n",
    "\n",
    "# Évaluation finale\n",
    "y_test_pred_final = final_model.predict(X_test)\n",
    "\n",
    "print(\"\\nRAPPORT DE CLASSIFICATION FINAL:\")\n",
    "print(classification_report(y_test, y_test_pred_final, \n",
    "                           target_names=['Classe 0', 'Classe 1', 'Classe 2']))\n",
    "\n",
    "# ====================================================================\n",
    "# SAUVEGARDE DES RÉSULTATS\n",
    "# ====================================================================\n",
    "print(\"\\n10. SAUVEGARDE DES RÉSULTATS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sauvegarder les résultats CSV\n",
    "results_df.to_csv(f'{MODEL_PATH}/class1_optimization_results.csv', index=False)\n",
    "print(\"Résultats sauvegardés: class1_optimization_results.csv\")\n",
    "\n",
    "# Sauvegarder la configuration optimale\n",
    "optimal_config = {\n",
    "    'baseline': {\n",
    "        'f1_overall': float(baseline_f1_overall),\n",
    "        'f1_class1': float(baseline_f1_class1),\n",
    "        'precision_class1': float(baseline_precision_class1),\n",
    "        'recall_class1': float(baseline_recall_class1)\n",
    "    },\n",
    "    'optimized': {\n",
    "        'multiplier': float(best_multiplier),\n",
    "        'f1_overall': float(results_df.loc[best_balanced_idx, 'f1_overall']),\n",
    "        'f1_class1': float(results_df.loc[best_balanced_idx, 'f1_class1']),\n",
    "        'precision_class1': float(results_df.loc[best_balanced_idx, 'precision_class1']),\n",
    "        'recall_class1': float(results_df.loc[best_balanced_idx, 'recall_class1']),\n",
    "        'class_weights': final_class_weights.tolist()\n",
    "    },\n",
    "    'improvements': {\n",
    "        'f1_overall': float(results_df.loc[best_balanced_idx, 'f1_overall'] - baseline_f1_overall),\n",
    "        'f1_class1': float(results_df.loc[best_balanced_idx, 'f1_class1'] - baseline_f1_class1)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_PATH}/class1_optimization_config.json', 'w') as f:\n",
    "    json.dump(optimal_config, f, indent=4)\n",
    "print(\"Configuration optimale: class1_optimization_config.json\")\n",
    "\n",
    "# Sauvegarder le modèle optimisé\n",
    "import joblib\n",
    "joblib.dump(final_model, f'{MODEL_PATH}/xgboost_optimized_class1.pkl')\n",
    "print(\"Modèle optimisé: xgboost_optimized_class1.pkl\")\n",
    "\n",
    "# ====================================================================\n",
    "# RÉSUMÉ FINAL\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ - OPTIMISATION CLASSE 1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMULTIPLICATEUR OPTIMAL: {best_multiplier:.1f}x\")\n",
    "\n",
    "print(\"\\nBASELINE:\")\n",
    "print(f\"  F1 Global: {baseline_f1_overall:.4f}\")\n",
    "print(f\"  F1 Classe 1: {baseline_f1_class1:.4f}\")\n",
    "\n",
    "print(\"\\nOPTIMISÉ:\")\n",
    "print(f\"  F1 Global: {results_df.loc[best_balanced_idx, 'f1_overall']:.4f}\")\n",
    "print(f\"  F1 Classe 1: {results_df.loc[best_balanced_idx, 'f1_class1']:.4f}\")\n",
    "\n",
    "print(\"\\nAMÉLIORATIONS:\")\n",
    "print(f\"  F1 Global: +{improvement_overall:.4f} ({improvement_overall/baseline_f1_overall*100:+.1f}%)\")\n",
    "print(f\"  F1 Classe 1: +{improvement_class1:.4f} ({improvement_class1/baseline_f1_class1*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FICHIERS GÉNÉRÉS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  1. class1_optimization_results.csv - Tous les résultats\")\n",
    "print(\"  2. class1_optimization_config.json - Configuration optimale\")\n",
    "print(\"  3. class1_weight_optimization.png - Visualisations\")\n",
    "print(\"  4. confusion_matrix_comparison.png - Comparaison matrices\")\n",
    "print(\"  5. xgboost_optimized_class1.pkl - Modèle optimisé\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ OPTIMISATION CLASSE 1 TERMINÉE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ce36d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBOOST - HYPERPARAMETER TUNING + FINAL TRAINING\n",
      "================================================================================\n",
      "\n",
      "1. VÉRIFICATION CUDA\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost version: 2.1.4\n",
      "Support GPU: OUI\n",
      "\n",
      "2. CHARGEMENT DES DONNÉES\n",
      "--------------------------------------------------------------------------------\n",
      "Train:        6694 samples,  13 features\n",
      "Validation:    956 samples,  13 features\n",
      "Test:          957 samples,  13 features\n",
      "\n",
      "Distribution des classes (Train):\n",
      "   False Positive      :  3387 (50.60%)\n",
      "   Candidate           :  1385 (20.69%)\n",
      "   Confirmed           :  1922 (28.71%)\n",
      "\n",
      "3. VÉRIFICATION DES FEATURES CRITIQUES\n",
      "--------------------------------------------------------------------------------\n",
      "   ✓ koi_duration\n",
      "   ✓ koi_duration_err1\n",
      "   ✓ koi_depth\n",
      "   ✓ koi_depth_err1\n",
      "   ✓ koi_model_snr\n",
      "\n",
      "4. FEATURE ENGINEERING\n",
      "--------------------------------------------------------------------------------\n",
      "   ✓ transit_depth_duration_ratio\n",
      "   ✓ snr_log, snr_squared\n",
      "   ✓ transit_depth_duration_ratio\n",
      "   ✓ snr_log, snr_squared\n",
      "   ✓ transit_depth_duration_ratio\n",
      "   ✓ snr_log, snr_squared\n",
      "\n",
      "Features finales: 16\n",
      "\n",
      "5. CALCUL DES POIDS DE CLASSES\n",
      "--------------------------------------------------------------------------------\n",
      "Poids calculés:\n",
      "   False Positive      : 0.5270x\n",
      "   Candidate           : 2.4166x [BOOST +50%]\n",
      "   Confirmed           : 1.1609x\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "Nombre total de combinaisons: 243\n",
      "\n",
      "Test en cours...\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.7 -> val_loss=0.524210\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.8 -> val_loss=0.524784\n",
      "lr=0.01, md=8, mcw=20, sub=0.7, col=0.9 -> val_loss=0.524776\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.7 -> val_loss=0.521107\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.8 -> val_loss=0.521326\n",
      "lr=0.01, md=8, mcw=20, sub=0.8, col=0.9 -> val_loss=0.521642\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.7 -> val_loss=0.518715\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.8 -> val_loss=0.518472\n",
      "lr=0.01, md=8, mcw=20, sub=0.9, col=0.9 -> val_loss=0.519052\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.7 -> val_loss=0.536754\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.8 -> val_loss=0.535917\n",
      "lr=0.01, md=8, mcw=30, sub=0.7, col=0.9 -> val_loss=0.536625\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.7 -> val_loss=0.532066\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.8 -> val_loss=0.532658\n",
      "lr=0.01, md=8, mcw=30, sub=0.8, col=0.9 -> val_loss=0.532145\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.7 -> val_loss=0.532022\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.8 -> val_loss=0.530245\n",
      "lr=0.01, md=8, mcw=30, sub=0.9, col=0.9 -> val_loss=0.529171\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.7 -> val_loss=0.546262\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.8 -> val_loss=0.546173\n",
      "lr=0.01, md=8, mcw=40, sub=0.7, col=0.9 -> val_loss=0.545812\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.7 -> val_loss=0.541418\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.8 -> val_loss=0.541449\n",
      "lr=0.01, md=8, mcw=40, sub=0.8, col=0.9 -> val_loss=0.541896\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.7 -> val_loss=0.539936\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.8 -> val_loss=0.539745\n",
      "lr=0.01, md=8, mcw=40, sub=0.9, col=0.9 -> val_loss=0.540617\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.7 -> val_loss=0.520303\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.8 -> val_loss=0.520097\n",
      "lr=0.01, md=10, mcw=20, sub=0.7, col=0.9 -> val_loss=0.520158\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.7 -> val_loss=0.515939\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.8 -> val_loss=0.514900\n",
      "lr=0.01, md=10, mcw=20, sub=0.8, col=0.9 -> val_loss=0.515709\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.7 -> val_loss=0.512466\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.8 -> val_loss=0.513205\n",
      "lr=0.01, md=10, mcw=20, sub=0.9, col=0.9 -> val_loss=0.513491\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.7 -> val_loss=0.533524\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.8 -> val_loss=0.534039\n",
      "lr=0.01, md=10, mcw=30, sub=0.7, col=0.9 -> val_loss=0.533377\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.7 -> val_loss=0.526856\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.8 -> val_loss=0.526724\n",
      "lr=0.01, md=10, mcw=30, sub=0.8, col=0.9 -> val_loss=0.527248\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.7 -> val_loss=0.526872\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.8 -> val_loss=0.526204\n",
      "lr=0.01, md=10, mcw=30, sub=0.9, col=0.9 -> val_loss=0.525819\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.7 -> val_loss=0.544507\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.8 -> val_loss=0.544718\n",
      "lr=0.01, md=10, mcw=40, sub=0.7, col=0.9 -> val_loss=0.543753\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.7 -> val_loss=0.538493\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.8 -> val_loss=0.538945\n",
      "lr=0.01, md=10, mcw=40, sub=0.8, col=0.9 -> val_loss=0.538915\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.7 -> val_loss=0.536098\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.8 -> val_loss=0.536040\n",
      "lr=0.01, md=10, mcw=40, sub=0.9, col=0.9 -> val_loss=0.537060\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.7 -> val_loss=0.518304\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.8 -> val_loss=0.517906\n",
      "lr=0.01, md=12, mcw=20, sub=0.7, col=0.9 -> val_loss=0.518708\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.7 -> val_loss=0.513341\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.8 -> val_loss=0.514306\n",
      "lr=0.01, md=12, mcw=20, sub=0.8, col=0.9 -> val_loss=0.515504\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.7 -> val_loss=0.511590\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.8 -> val_loss=0.510408\n",
      "lr=0.01, md=12, mcw=20, sub=0.9, col=0.9 -> val_loss=0.510800\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.7 -> val_loss=0.531850\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.8 -> val_loss=0.531707\n",
      "lr=0.01, md=12, mcw=30, sub=0.7, col=0.9 -> val_loss=0.531653\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.7 -> val_loss=0.526324\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.8 -> val_loss=0.525782\n",
      "lr=0.01, md=12, mcw=30, sub=0.8, col=0.9 -> val_loss=0.526616\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.7 -> val_loss=0.525205\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.8 -> val_loss=0.523255\n",
      "lr=0.01, md=12, mcw=30, sub=0.9, col=0.9 -> val_loss=0.522550\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.7 -> val_loss=0.543225\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.8 -> val_loss=0.543626\n",
      "lr=0.01, md=12, mcw=40, sub=0.7, col=0.9 -> val_loss=0.542568\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.7 -> val_loss=0.537665\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.8 -> val_loss=0.537632\n",
      "lr=0.01, md=12, mcw=40, sub=0.8, col=0.9 -> val_loss=0.538633\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.7 -> val_loss=0.534628\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.8 -> val_loss=0.533835\n",
      "lr=0.01, md=12, mcw=40, sub=0.9, col=0.9 -> val_loss=0.535306\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.7 -> val_loss=0.513707\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.8 -> val_loss=0.515191\n",
      "lr=0.03, md=8, mcw=20, sub=0.7, col=0.9 -> val_loss=0.513226\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.7 -> val_loss=0.509162\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.8 -> val_loss=0.509344\n",
      "lr=0.03, md=8, mcw=20, sub=0.8, col=0.9 -> val_loss=0.509845\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.7 -> val_loss=0.508838\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.8 -> val_loss=0.507268\n",
      "lr=0.03, md=8, mcw=20, sub=0.9, col=0.9 -> val_loss=0.505604\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.7 -> val_loss=0.516407\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.8 -> val_loss=0.517638\n",
      "lr=0.03, md=8, mcw=30, sub=0.7, col=0.9 -> val_loss=0.517060\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.7 -> val_loss=0.516586\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.8 -> val_loss=0.516047\n",
      "lr=0.03, md=8, mcw=30, sub=0.8, col=0.9 -> val_loss=0.519067\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.7 -> val_loss=0.514722\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.8 -> val_loss=0.515062\n",
      "lr=0.03, md=8, mcw=30, sub=0.9, col=0.9 -> val_loss=0.516366\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.7 -> val_loss=0.525365\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.8 -> val_loss=0.526378\n",
      "lr=0.03, md=8, mcw=40, sub=0.7, col=0.9 -> val_loss=0.524599\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.7 -> val_loss=0.520110\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.8 -> val_loss=0.520396\n",
      "lr=0.03, md=8, mcw=40, sub=0.8, col=0.9 -> val_loss=0.522341\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.7 -> val_loss=0.520863\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.8 -> val_loss=0.520839\n",
      "lr=0.03, md=8, mcw=40, sub=0.9, col=0.9 -> val_loss=0.524617\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.7 -> val_loss=0.513842\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.8 -> val_loss=0.514559\n",
      "lr=0.03, md=10, mcw=20, sub=0.7, col=0.9 -> val_loss=0.514632\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.7 -> val_loss=0.507590\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.8 -> val_loss=0.510949\n",
      "lr=0.03, md=10, mcw=20, sub=0.8, col=0.9 -> val_loss=0.510236\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.7 -> val_loss=0.505456\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.8 -> val_loss=0.504917\n",
      "lr=0.03, md=10, mcw=20, sub=0.9, col=0.9 -> val_loss=0.508716\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.7 -> val_loss=0.519385\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.8 -> val_loss=0.517275\n",
      "lr=0.03, md=10, mcw=30, sub=0.7, col=0.9 -> val_loss=0.517982\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.7 -> val_loss=0.514802\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.8 -> val_loss=0.517705\n",
      "lr=0.03, md=10, mcw=30, sub=0.8, col=0.9 -> val_loss=0.517692\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.7 -> val_loss=0.514578\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.8 -> val_loss=0.513019\n",
      "lr=0.03, md=10, mcw=30, sub=0.9, col=0.9 -> val_loss=0.514261\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.7 -> val_loss=0.522500\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.8 -> val_loss=0.527736\n",
      "lr=0.03, md=10, mcw=40, sub=0.7, col=0.9 -> val_loss=0.525799\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.7 -> val_loss=0.519361\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.8 -> val_loss=0.525034\n",
      "lr=0.03, md=10, mcw=40, sub=0.8, col=0.9 -> val_loss=0.524026\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.7 -> val_loss=0.521464\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.8 -> val_loss=0.520679\n",
      "lr=0.03, md=10, mcw=40, sub=0.9, col=0.9 -> val_loss=0.519370\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.7 -> val_loss=0.513277\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.8 -> val_loss=0.514442\n",
      "lr=0.03, md=12, mcw=20, sub=0.7, col=0.9 -> val_loss=0.506516\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.7 -> val_loss=0.509653\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.8 -> val_loss=0.509568\n",
      "lr=0.03, md=12, mcw=20, sub=0.8, col=0.9 -> val_loss=0.509782\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.7 -> val_loss=0.505660\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.8 -> val_loss=0.503175\n",
      "lr=0.03, md=12, mcw=20, sub=0.9, col=0.9 -> val_loss=0.509348\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.7 -> val_loss=0.521716\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.8 -> val_loss=0.520149\n",
      "lr=0.03, md=12, mcw=30, sub=0.7, col=0.9 -> val_loss=0.520016\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.7 -> val_loss=0.515904\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.8 -> val_loss=0.518584\n",
      "lr=0.03, md=12, mcw=30, sub=0.8, col=0.9 -> val_loss=0.520758\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.7 -> val_loss=0.515022\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.8 -> val_loss=0.515735\n",
      "lr=0.03, md=12, mcw=30, sub=0.9, col=0.9 -> val_loss=0.517144\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.7 -> val_loss=0.521210\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.8 -> val_loss=0.524868\n",
      "lr=0.03, md=12, mcw=40, sub=0.7, col=0.9 -> val_loss=0.524380\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.7 -> val_loss=0.524384\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.8 -> val_loss=0.521908\n",
      "lr=0.03, md=12, mcw=40, sub=0.8, col=0.9 -> val_loss=0.521934\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.7 -> val_loss=0.522728\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.8 -> val_loss=0.520983\n",
      "lr=0.03, md=12, mcw=40, sub=0.9, col=0.9 -> val_loss=0.519148\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.7 -> val_loss=0.516638\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.8 -> val_loss=0.514921\n",
      "lr=0.05, md=8, mcw=20, sub=0.7, col=0.9 -> val_loss=0.518153\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.7 -> val_loss=0.510641\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.8 -> val_loss=0.508430\n",
      "lr=0.05, md=8, mcw=20, sub=0.8, col=0.9 -> val_loss=0.511785\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.7 -> val_loss=0.508853\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.8 -> val_loss=0.510059\n",
      "lr=0.05, md=8, mcw=20, sub=0.9, col=0.9 -> val_loss=0.507962\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.7 -> val_loss=0.514920\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.8 -> val_loss=0.519620\n",
      "lr=0.05, md=8, mcw=30, sub=0.7, col=0.9 -> val_loss=0.524185\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.7 -> val_loss=0.516496\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.8 -> val_loss=0.517420\n",
      "lr=0.05, md=8, mcw=30, sub=0.8, col=0.9 -> val_loss=0.519016\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.7 -> val_loss=0.516834\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.8 -> val_loss=0.516525\n",
      "lr=0.05, md=8, mcw=30, sub=0.9, col=0.9 -> val_loss=0.519048\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.7 -> val_loss=0.523119\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.8 -> val_loss=0.521994\n",
      "lr=0.05, md=8, mcw=40, sub=0.7, col=0.9 -> val_loss=0.531735\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.7 -> val_loss=0.524668\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.8 -> val_loss=0.521441\n",
      "lr=0.05, md=8, mcw=40, sub=0.8, col=0.9 -> val_loss=0.521421\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.7 -> val_loss=0.524933\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.8 -> val_loss=0.521966\n",
      "lr=0.05, md=8, mcw=40, sub=0.9, col=0.9 -> val_loss=0.524196\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.7 -> val_loss=0.516111\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.8 -> val_loss=0.515718\n",
      "lr=0.05, md=10, mcw=20, sub=0.7, col=0.9 -> val_loss=0.517711\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.7 -> val_loss=0.509322\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.8 -> val_loss=0.511449\n",
      "lr=0.05, md=10, mcw=20, sub=0.8, col=0.9 -> val_loss=0.514700\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.7 -> val_loss=0.507483\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.8 -> val_loss=0.511586\n",
      "lr=0.05, md=10, mcw=20, sub=0.9, col=0.9 -> val_loss=0.506006\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.7 -> val_loss=0.523059\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.8 -> val_loss=0.521069\n",
      "lr=0.05, md=10, mcw=30, sub=0.7, col=0.9 -> val_loss=0.518336\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.7 -> val_loss=0.512905\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.8 -> val_loss=0.515509\n",
      "lr=0.05, md=10, mcw=30, sub=0.8, col=0.9 -> val_loss=0.521835\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.7 -> val_loss=0.513508\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.8 -> val_loss=0.518934\n",
      "lr=0.05, md=10, mcw=30, sub=0.9, col=0.9 -> val_loss=0.515387\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.7 -> val_loss=0.524457\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.8 -> val_loss=0.526444\n",
      "lr=0.05, md=10, mcw=40, sub=0.7, col=0.9 -> val_loss=0.522074\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.7 -> val_loss=0.521788\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.8 -> val_loss=0.521513\n",
      "lr=0.05, md=10, mcw=40, sub=0.8, col=0.9 -> val_loss=0.521662\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.7 -> val_loss=0.526673\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.8 -> val_loss=0.522573\n",
      "lr=0.05, md=10, mcw=40, sub=0.9, col=0.9 -> val_loss=0.523063\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.7 -> val_loss=0.514303\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.8 -> val_loss=0.515540\n",
      "lr=0.05, md=12, mcw=20, sub=0.7, col=0.9 -> val_loss=0.516867\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.7 -> val_loss=0.508973\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.8 -> val_loss=0.511565\n",
      "lr=0.05, md=12, mcw=20, sub=0.8, col=0.9 -> val_loss=0.510906\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.7 -> val_loss=0.510697\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.8 -> val_loss=0.512541\n",
      "lr=0.05, md=12, mcw=20, sub=0.9, col=0.9 -> val_loss=0.513042\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.7 -> val_loss=0.519603\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.8 -> val_loss=0.522961\n",
      "lr=0.05, md=12, mcw=30, sub=0.7, col=0.9 -> val_loss=0.521126\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.7 -> val_loss=0.517590\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.8 -> val_loss=0.517317\n",
      "lr=0.05, md=12, mcw=30, sub=0.8, col=0.9 -> val_loss=0.520066\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.7 -> val_loss=0.516811\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.8 -> val_loss=0.518056\n",
      "lr=0.05, md=12, mcw=30, sub=0.9, col=0.9 -> val_loss=0.519112\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.7 -> val_loss=0.523000\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.8 -> val_loss=0.523120\n",
      "lr=0.05, md=12, mcw=40, sub=0.7, col=0.9 -> val_loss=0.522264\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.7 -> val_loss=0.520839\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.8 -> val_loss=0.526731\n",
      "lr=0.05, md=12, mcw=40, sub=0.8, col=0.9 -> val_loss=0.523746\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.7 -> val_loss=0.523381\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.8 -> val_loss=0.524742\n",
      "lr=0.05, md=12, mcw=40, sub=0.9, col=0.9 -> val_loss=0.525943\n",
      "\n",
      "================================================================================\n",
      "MEILLEURS HYPERPARAMÈTRES TROUVÉS\n",
      "================================================================================\n",
      "\n",
      "Meilleur score validation: 0.503175\n",
      "\n",
      "Hyperparamètres:\n",
      "   learning_rate:     0.03\n",
      "   max_depth:         12\n",
      "   min_child_weight:  20\n",
      "   subsample:         0.9\n",
      "   colsample_bytree:  0.8\n",
      "\n",
      "Résultats sauvegardés: C:/Users/chaym/Desktop/NasaChallenge/models/xgboost_model/hyperparameter_tuning_results.csv\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: ENTRAÎNEMENT FINAL\n",
      "================================================================================\n",
      "\n",
      "Dataset combiné (Train+Val): 7650 samples\n",
      "\n",
      "Entraînement final en cours...\n",
      "\n",
      "[0]\ttrain-mlogloss:1.07565\ttest-mlogloss:1.07894\n",
      "[100]\ttrain-mlogloss:0.42629\ttest-mlogloss:0.60491\n",
      "[200]\ttrain-mlogloss:0.32540\ttest-mlogloss:0.57084\n",
      "[300]\ttrain-mlogloss:0.27225\ttest-mlogloss:0.56023\n",
      "[400]\ttrain-mlogloss:0.23504\ttest-mlogloss:0.55551\n",
      "[500]\ttrain-mlogloss:0.20657\ttest-mlogloss:0.55497\n",
      "[576]\ttrain-mlogloss:0.18892\ttest-mlogloss:0.55641\n",
      "\n",
      "Entraînement terminé!\n",
      "   Best Iteration: 477\n",
      "   Best Test Loss: 0.554368\n",
      "\n",
      "================================================================================\n",
      "ÉVALUATION FINALE SUR TEST SET\n",
      "================================================================================\n",
      "\n",
      "Performance Metrics:\n",
      "   Accuracy:           0.7503 (75.03%)\n",
      "   Precision (Avg):    0.7834\n",
      "   Recall (Avg):       0.7503\n",
      "   F1-Score (Macro):   0.7325\n",
      "   F1-Score (Weighted):0.7596\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive     0.8822    0.7273    0.7973       484\n",
      "     Candidate     0.4892    0.6869    0.5714       198\n",
      "     Confirmed     0.8214    0.8364    0.8288       275\n",
      "\n",
      "      accuracy                         0.7503       957\n",
      "     macro avg     0.7309    0.7502    0.7325       957\n",
      "  weighted avg     0.7834    0.7503    0.7596       957\n",
      "\n",
      "\n",
      "PERFORMANCE CLASSE 1 (Candidate): 0.6869 (68.69%)\n",
      "\n",
      "9. VISUALISATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Matrice de confusion: confusion_matrix_final.png\n",
      "\n",
      "TOP 15 FEATURES:\n",
      "          feature  importance\n",
      "    koi_model_snr    7.941135\n",
      "          snr_log    6.574655\n",
      "         koi_prad    6.051306\n",
      "      snr_squared    5.325435\n",
      "koi_duration_err1    3.798442\n",
      "       koi_period    3.554400\n",
      "    koi_prad_err1    3.332065\n",
      "  koi_period_err1    2.356472\n",
      "     koi_duration    2.323794\n",
      "    koi_prad_err2    2.054319\n",
      "    koi_srad_err1    1.858565\n",
      "   koi_depth_err1    1.794445\n",
      "        koi_depth    1.647894\n",
      "    koi_srad_err2    1.616680\n",
      "         koi_srad    1.605003\n",
      "Feature importance: feature_importance_final.png\n",
      "\n",
      "10. SAUVEGARDE\n",
      "--------------------------------------------------------------------------------\n",
      "Modèle: exoplanet_xgboost_final.json\n",
      "Métadonnées: model_metadata_final.json\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLET TERMINÉ\n",
      "================================================================================\n",
      "\n",
      "Fichiers dans C:/Users/chaym/Desktop/NasaChallenge/models/xgboost_model/:\n",
      "   - hyperparameter_tuning_results.csv\n",
      "   - exoplanet_xgboost_final.json\n",
      "   - model_metadata_final.json\n",
      "   - confusion_matrix_final.png\n",
      "   - feature_importance_final.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from itertools import product\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# CONFIGURATION DES CHEMINS\n",
    "BASE_PATH = 'C:/Users/chaym/Desktop/NasaChallenge'\n",
    "DATA_PATH = f'{BASE_PATH}/data/processed'\n",
    "MODEL_PATH = f'{BASE_PATH}/models/xgboost_model'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"XGBOOST - HYPERPARAMETER TUNING + FINAL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# 1. VÉRIFICATION CUDA\n",
    "# ====================================================================\n",
    "print(\"\\n1. VÉRIFICATION CUDA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "build_info = xgb.build_info()\n",
    "\n",
    "if not build_info.get('USE_CUDA', False):\n",
    "    print(\"ATTENTION: XGBoost n'a pas été compilé avec CUDA\")\n",
    "    print(\"Le modèle utilisera le CPU\")\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    print(\"Support GPU: OUI\")\n",
    "    device = 'cuda'\n",
    "\n",
    "# ====================================================================\n",
    "# 2. CHARGEMENT DES DONNÉES\n",
    "# ====================================================================\n",
    "print(\"\\n2. CHARGEMENT DES DONNÉES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').squeeze()\n",
    "\n",
    "X_validate = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "y_validate = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').squeeze()\n",
    "\n",
    "X_test = pd.read_csv(f'{DATA_PATH}/step6_X_test.csv')\n",
    "y_test = pd.read_csv(f'{DATA_PATH}/step6_y_test.csv').squeeze()\n",
    "\n",
    "class_names = {0: 'False Positive', 1: 'Candidate', 2: 'Confirmed'}\n",
    "\n",
    "print(f\"Train:      {X_train.shape[0]:>6} samples, {X_train.shape[1]:>3} features\")\n",
    "print(f\"Validation: {X_validate.shape[0]:>6} samples, {X_validate.shape[1]:>3} features\")\n",
    "print(f\"Test:       {X_test.shape[0]:>6} samples, {X_test.shape[1]:>3} features\")\n",
    "\n",
    "# Distribution des classes\n",
    "print(\"\\nDistribution des classes (Train):\")\n",
    "for cls in [0, 1, 2]:\n",
    "    count = (y_train == cls).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"   {class_names[cls]:20s}: {count:>5} ({pct:>5.2f}%)\")\n",
    "\n",
    "# ====================================================================\n",
    "# 3. VÉRIFICATION DES FEATURES\n",
    "# ====================================================================\n",
    "print(\"\\n3. VÉRIFICATION DES FEATURES CRITIQUES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "required_features = [\n",
    "    'koi_duration', 'koi_duration_err1',\n",
    "    'koi_depth', 'koi_depth_err1',\n",
    "    'koi_model_snr'\n",
    "]\n",
    "\n",
    "for feat in required_features:\n",
    "    if feat in X_train.columns:\n",
    "        print(f\"   ✓ {feat}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Feature manquante: {feat}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 4. FEATURE ENGINEERING\n",
    "# ====================================================================\n",
    "print(\"\\n4. FEATURE ENGINEERING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def engineer_transit_features(X):\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    if 'koi_depth' in X.columns and 'koi_duration' in X.columns:\n",
    "        X_enhanced['transit_depth_duration_ratio'] = (\n",
    "            X['koi_depth'] / (X['koi_duration'] + 1e-6)\n",
    "        )\n",
    "        print(\"   ✓ transit_depth_duration_ratio\")\n",
    "    \n",
    "    if 'koi_model_snr' in X.columns:\n",
    "        X_enhanced['snr_log'] = np.log1p(X['koi_model_snr'])\n",
    "        X_enhanced['snr_squared'] = X['koi_model_snr'] ** 2\n",
    "        print(\"   ✓ snr_log, snr_squared\")\n",
    "    \n",
    "    return X_enhanced\n",
    "\n",
    "X_train_enh = engineer_transit_features(X_train)\n",
    "X_val_enh = engineer_transit_features(X_validate)\n",
    "X_test_enh = engineer_transit_features(X_test)\n",
    "\n",
    "print(f\"\\nFeatures finales: {X_train_enh.shape[1]}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 5. CALCUL DES POIDS DE CLASSES\n",
    "# ====================================================================\n",
    "print(\"\\n5. CALCUL DES POIDS DE CLASSES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_classes = np.array([0, 1, 2])\n",
    "auto_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=all_classes, \n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = {\n",
    "    0: auto_weights[0] * 0.8,\n",
    "    1: auto_weights[1] * 1.5,  # BOOST Candidate\n",
    "    2: auto_weights[2] * 1.0\n",
    "}\n",
    "\n",
    "print(\"Poids calculés:\")\n",
    "for cls, weight in class_weights.items():\n",
    "    boost = \" [BOOST +50%]\" if cls == 1 else \"\"\n",
    "    print(f\"   {class_names[cls]:20s}: {weight:.4f}x{boost}\")\n",
    "\n",
    "sample_weights_train = np.array([class_weights[cls] for cls in y_train])\n",
    "\n",
    "# ====================================================================\n",
    "# 6. PHASE 1: HYPERPARAMETER TUNING (Train/Val)\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'max_depth': [8, 10, 12],\n",
    "    'min_child_weight': [20, 30, 40],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "print(f\"\\nNombre total de combinaisons: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"\\nTest en cours...\")\n",
    "\n",
    "# Préparer les datasets pour tuning\n",
    "dtrain_tune = xgb.DMatrix(X_train_enh, label=y_train, weight=sample_weights_train)\n",
    "dval_tune = xgb.DMatrix(X_val_enh, label=y_validate)\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "# Grid Search\n",
    "for lr, md, mcw, sub, col in product(\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_child_weight'],\n",
    "    param_grid['subsample'],\n",
    "    param_grid['colsample_bytree']\n",
    "):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'tree_method': 'hist' if device == 'cpu' else 'gpu_hist',\n",
    "        'device': device,\n",
    "        'learning_rate': lr,\n",
    "        'max_depth': md,\n",
    "        'min_child_weight': mcw,\n",
    "        'subsample': sub,\n",
    "        'colsample_bytree': col,\n",
    "        'alpha': 0.1,\n",
    "        'lambda': 0.1,\n",
    "        'gamma': 0.001,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Entraînement\n",
    "    evals = [(dtrain_tune, 'train'), (dval_tune, 'valid')]\n",
    "    model_temp = xgb.train(\n",
    "        params,\n",
    "        dtrain_tune,\n",
    "        num_boost_round=1000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    val_score = model_temp.best_score\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'max_depth': md,\n",
    "        'min_child_weight': mcw,\n",
    "        'subsample': sub,\n",
    "        'colsample_bytree': col,\n",
    "        'val_score': val_score,\n",
    "        'best_iteration': model_temp.best_iteration\n",
    "    })\n",
    "    \n",
    "    if val_score < best_score:\n",
    "        best_score = val_score\n",
    "        best_params = params.copy()\n",
    "    \n",
    "    print(f\"lr={lr:.2f}, md={md}, mcw={mcw}, sub={sub:.1f}, col={col:.1f} -> val_loss={val_score:.6f}\")\n",
    "\n",
    "# Afficher les meilleurs résultats\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEILLEURS HYPERPARAMÈTRES TROUVÉS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMeilleur score validation: {best_score:.6f}\")\n",
    "print(\"\\nHyperparamètres:\")\n",
    "print(f\"   learning_rate:     {best_params['learning_rate']}\")\n",
    "print(f\"   max_depth:         {best_params['max_depth']}\")\n",
    "print(f\"   min_child_weight:  {best_params['min_child_weight']}\")\n",
    "print(f\"   subsample:         {best_params['subsample']}\")\n",
    "print(f\"   colsample_bytree:  {best_params['colsample_bytree']}\")\n",
    "\n",
    "# Sauvegarder les résultats du tuning\n",
    "results_df = pd.DataFrame(results).sort_values('val_score')\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "results_df.to_csv(f'{MODEL_PATH}/hyperparameter_tuning_results.csv', index=False)\n",
    "print(f\"\\nRésultats sauvegardés: {MODEL_PATH}/hyperparameter_tuning_results.csv\")\n",
    "\n",
    "# ====================================================================\n",
    "# 7. PHASE 2: ENTRAÎNEMENT FINAL (Train+Val) / Test\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: ENTRAÎNEMENT FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combiner Train et Val pour l'entraînement final\n",
    "X_train_val = pd.concat([X_train_enh, X_val_enh], axis=0)\n",
    "y_train_val = pd.concat([y_train, y_validate], axis=0)\n",
    "\n",
    "# Recalculer les poids pour le dataset combiné\n",
    "sample_weights_combined = np.array([class_weights[cls] for cls in y_train_val])\n",
    "\n",
    "print(f\"\\nDataset combiné (Train+Val): {X_train_val.shape[0]} samples\")\n",
    "\n",
    "# Préparer les datasets finaux\n",
    "dtrain_final = xgb.DMatrix(X_train_val, label=y_train_val, weight=sample_weights_combined)\n",
    "dtest_final = xgb.DMatrix(X_test_enh, label=y_test)\n",
    "\n",
    "# Entraînement final avec meilleurs hyperparamètres\n",
    "print(\"\\nEntraînement final en cours...\\n\")\n",
    "\n",
    "evals_final = [(dtrain_final, 'train'), (dtest_final, 'test')]\n",
    "\n",
    "model_final = xgb.train(\n",
    "    best_params,\n",
    "    dtrain_final,\n",
    "    num_boost_round=3000,\n",
    "    evals=evals_final,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "print(f\"\\nEntraînement terminé!\")\n",
    "print(f\"   Best Iteration: {model_final.best_iteration}\")\n",
    "print(f\"   Best Test Loss: {model_final.best_score:.6f}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 8. ÉVALUATION FINALE SUR TEST SET\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ÉVALUATION FINALE SUR TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_test_pred_proba = model_final.predict(dtest_final, iteration_range=(0, model_final.best_iteration))\n",
    "y_test_pred = y_test_pred_proba.argmax(axis=1)\n",
    "\n",
    "# Métriques\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"   Accuracy:           {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision (Avg):    {test_precision:.4f}\")\n",
    "print(f\"   Recall (Avg):       {test_recall:.4f}\")\n",
    "print(f\"   F1-Score (Macro):   {test_f1_macro:.4f}\")\n",
    "print(f\"   F1-Score (Weighted):{test_f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_test_pred,\n",
    "    target_names=['False Positive', 'Candidate', 'Confirmed'],\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Performance classe 1\n",
    "class1_mask = y_test == 1\n",
    "if class1_mask.sum() > 0:\n",
    "    class1_acc = accuracy_score(y_test[class1_mask], y_test_pred[class1_mask])\n",
    "    print(f\"\\nPERFORMANCE CLASSE 1 (Candidate): {class1_acc:.4f} ({class1_acc*100:.2f}%)\")\n",
    "\n",
    "# ====================================================================\n",
    "# 9. VISUALISATIONS\n",
    "# ====================================================================\n",
    "print(\"\\n9. VISUALISATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['False Positive', 'Candidate', 'Confirmed'],\n",
    "            yticklabels=['False Positive', 'Candidate', 'Confirmed'])\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/confusion_matrix_final.png', dpi=300)\n",
    "print(\"Matrice de confusion: confusion_matrix_final.png\")\n",
    "plt.close()\n",
    "\n",
    "# Feature importance\n",
    "importance_dict = model_final.get_score(importance_type='gain')\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': list(importance_dict.keys()),\n",
    "    'importance': list(importance_dict.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTOP 15 FEATURES:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(top_20['feature'], top_20['importance'], color='steelblue')\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Features', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/feature_importance_final.png', dpi=300)\n",
    "print(\"Feature importance: feature_importance_final.png\")\n",
    "plt.close()\n",
    "\n",
    "# ====================================================================\n",
    "# 10. SAUVEGARDE\n",
    "# ====================================================================\n",
    "print(\"\\n10. SAUVEGARDE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "model_final.save_model(f'{MODEL_PATH}/exoplanet_xgboost_final.json')\n",
    "print(f\"Modèle: exoplanet_xgboost_final.json\")\n",
    "\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'xgboost_version': xgb.__version__,\n",
    "    'device': device,\n",
    "    'training_strategy': 'Train+Val combined for final model',\n",
    "    'best_hyperparameters': {\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'max_depth': best_params['max_depth'],\n",
    "        'min_child_weight': best_params['min_child_weight'],\n",
    "        'subsample': best_params['subsample'],\n",
    "        'colsample_bytree': best_params['colsample_bytree']\n",
    "    },\n",
    "    'best_iteration': int(model_final.best_iteration),\n",
    "    'best_score': float(model_final.best_score),\n",
    "    'test_metrics': {\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'precision': float(test_precision),\n",
    "        'recall': float(test_recall),\n",
    "        'f1_macro': float(test_f1_macro),\n",
    "        'f1_weighted': float(test_f1_weighted)\n",
    "    },\n",
    "    'class_weights': {str(k): float(v) for k, v in class_weights.items()},\n",
    "    'features_used': [\n",
    "        'koi_duration', 'koi_duration_err1',\n",
    "        'koi_depth', 'koi_depth_err1',\n",
    "        'koi_model_snr', 'snr_log', 'snr_squared',\n",
    "        'transit_depth_duration_ratio'\n",
    "    ],\n",
    "    'training_samples': int(X_train_val.shape[0]),\n",
    "    'test_samples': int(X_test_enh.shape[0])\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_PATH}/model_metadata_final.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(\"Métadonnées: model_metadata_final.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLET TERMINÉ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFichiers dans {MODEL_PATH}/:\")\n",
    "print(\"   - hyperparameter_tuning_results.csv\")\n",
    "print(\"   - exoplanet_xgboost_final.json\")\n",
    "print(\"   - model_metadata_final.json\")\n",
    "print(\"   - confusion_matrix_final.png\")\n",
    "print(\"   - feature_importance_final.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
